{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:04.228686Z",
     "start_time": "2024-12-01T12:58:59.035327Z"
    }
   },
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:04.260426Z",
     "start_time": "2024-12-01T12:59:04.228686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    with open(filename, mode='rt', encoding='utf-8') as file:\n",
    "        # read all text\n",
    "        text = file.read()\n",
    "    return text\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "\tlines = doc.strip().split('\\n')\n",
    "\tpairs = [line.split('\\t') for line in  lines]\n",
    "\treturn pairs\n",
    "\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars from each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ],
   "id": "d8b77284dc7cdd86",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:09.230575Z",
     "start_time": "2024-12-01T12:59:04.261291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filename = \"D:\\data\\deu.txt\"\n",
    "doc = load_doc(filename)\n",
    "pairs = to_pairs(doc)\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "for i in range(100):\n",
    "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ],
   "id": "8274a1609c3e93ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hi] => [hallo]\n",
      "[hi] => [gru gott]\n",
      "[run] => [lauf]\n",
      "[wow] => [potzdonner]\n",
      "[wow] => [donnerwetter]\n",
      "[fire] => [feuer]\n",
      "[help] => [hilfe]\n",
      "[help] => [zu hulf]\n",
      "[stop] => [stopp]\n",
      "[wait] => [warte]\n",
      "[hello] => [hallo]\n",
      "[i try] => [ich probiere es]\n",
      "[i won] => [ich hab gewonnen]\n",
      "[i won] => [ich habe gewonnen]\n",
      "[smile] => [lacheln]\n",
      "[cheers] => [zum wohl]\n",
      "[freeze] => [keine bewegung]\n",
      "[freeze] => [stehenbleiben]\n",
      "[got it] => [verstanden]\n",
      "[got it] => [einverstanden]\n",
      "[he ran] => [er rannte]\n",
      "[he ran] => [er lief]\n",
      "[hop in] => [mach mit]\n",
      "[hug me] => [druck mich]\n",
      "[hug me] => [nimm mich in den arm]\n",
      "[hug me] => [umarme mich]\n",
      "[i fell] => [ich fiel]\n",
      "[i fell] => [ich fiel hin]\n",
      "[i fell] => [ich sturzte]\n",
      "[i fell] => [ich bin hingefallen]\n",
      "[i fell] => [ich bin gesturzt]\n",
      "[i know] => [ich wei]\n",
      "[i lied] => [ich habe gelogen]\n",
      "[i lost] => [ich habe verloren]\n",
      "[im] => [ich bin jahre alt]\n",
      "[im] => [ich bin]\n",
      "[im ok] => [mir gehts gut]\n",
      "[im ok] => [es geht mir gut]\n",
      "[no way] => [unmoglich]\n",
      "[no way] => [das gibts doch nicht]\n",
      "[no way] => [ausgeschlossen]\n",
      "[no way] => [in keinster weise]\n",
      "[really] => [wirklich]\n",
      "[really] => [echt]\n",
      "[really] => [im ernst]\n",
      "[thanks] => [danke]\n",
      "[try it] => [versuchs]\n",
      "[why me] => [warum ich]\n",
      "[ask tom] => [frag tom]\n",
      "[ask tom] => [fragen sie tom]\n",
      "[ask tom] => [fragt tom]\n",
      "[be cool] => [entspann dich]\n",
      "[be fair] => [sei nicht ungerecht]\n",
      "[be fair] => [sei fair]\n",
      "[be nice] => [sei nett]\n",
      "[be nice] => [seien sie nett]\n",
      "[beat it] => [geh weg]\n",
      "[beat it] => [hau ab]\n",
      "[beat it] => [verschwinde]\n",
      "[beat it] => [verdufte]\n",
      "[beat it] => [mach dich fort]\n",
      "[beat it] => [zieh leine]\n",
      "[beat it] => [mach dich vom acker]\n",
      "[beat it] => [verzieh dich]\n",
      "[beat it] => [verkrumele dich]\n",
      "[beat it] => [troll dich]\n",
      "[beat it] => [zisch ab]\n",
      "[beat it] => [pack dich]\n",
      "[beat it] => [mach ne fliege]\n",
      "[beat it] => [schwirr ab]\n",
      "[beat it] => [mach die sause]\n",
      "[beat it] => [scher dich weg]\n",
      "[beat it] => [scher dich fort]\n",
      "[call me] => [ruf mich an]\n",
      "[come in] => [komm herein]\n",
      "[come in] => [herein]\n",
      "[come on] => [komm]\n",
      "[come on] => [kommt]\n",
      "[come on] => [mach schon]\n",
      "[come on] => [macht schon]\n",
      "[get out] => [raus]\n",
      "[go away] => [geh weg]\n",
      "[go away] => [hau ab]\n",
      "[go away] => [verschwinde]\n",
      "[go away] => [verdufte]\n",
      "[go away] => [mach dich fort]\n",
      "[go away] => [zieh leine]\n",
      "[go away] => [mach dich vom acker]\n",
      "[go away] => [verzieh dich]\n",
      "[go away] => [verkrumele dich]\n",
      "[go away] => [troll dich]\n",
      "[go away] => [zisch ab]\n",
      "[go away] => [pack dich]\n",
      "[go away] => [mach ne fliege]\n",
      "[go away] => [schwirr ab]\n",
      "[go away] => [mach die sause]\n",
      "[go away] => [scher dich weg]\n",
      "[go away] => [scher dich fort]\n",
      "[go away] => [geh weg]\n",
      "[go away] => [verpiss dich]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:09.262206Z",
     "start_time": "2024-12-01T12:59:09.233751Z"
    }
   },
   "cell_type": "code",
   "source": "len(clean_pairs)",
   "id": "a3616795eb367fd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152820"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:09.356780Z",
     "start_time": "2024-12-01T12:59:09.262206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in clean_pairs:\n",
    "\tif len(p) !=2:\n",
    "\t\tprint(len(p))"
   ],
   "id": "95cf2e8c6d71acd9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:11.191857Z",
     "start_time": "2024-12-01T12:59:09.356780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_freq = Counter()\n",
    "for pair in clean_pairs:\n",
    "\tword_freq.update(pair[0].split())\n",
    "\tword_freq.update(pair[1].split())\n",
    "word_freq"
   ],
   "id": "e09f4a6f94f61024",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'tom': 60434,\n",
       "         'ich': 40537,\n",
       "         'the': 37212,\n",
       "         'to': 34851,\n",
       "         'you': 34260,\n",
       "         'i': 32855,\n",
       "         'a': 24014,\n",
       "         'in': 21454,\n",
       "         'ist': 21346,\n",
       "         'nicht': 21081,\n",
       "         'sie': 19804,\n",
       "         'is': 18781,\n",
       "         'du': 17570,\n",
       "         'das': 17320,\n",
       "         'was': 16128,\n",
       "         'zu': 15622,\n",
       "         'die': 14282,\n",
       "         'es': 13890,\n",
       "         'er': 13412,\n",
       "         'he': 12141,\n",
       "         'of': 11448,\n",
       "         'der': 11165,\n",
       "         'it': 10381,\n",
       "         'that': 10358,\n",
       "         'do': 9481,\n",
       "         'have': 9247,\n",
       "         'this': 9100,\n",
       "         'hat': 9077,\n",
       "         'me': 8858,\n",
       "         'ein': 8730,\n",
       "         'dass': 8274,\n",
       "         'for': 7841,\n",
       "         'im': 7812,\n",
       "         'my': 7684,\n",
       "         'wir': 7604,\n",
       "         'habe': 7184,\n",
       "         'an': 7087,\n",
       "         'mary': 7063,\n",
       "         'mir': 6942,\n",
       "         'dont': 6851,\n",
       "         'auf': 6567,\n",
       "         'sich': 6529,\n",
       "         'your': 6405,\n",
       "         'mit': 6402,\n",
       "         'are': 6392,\n",
       "         'what': 6297,\n",
       "         'his': 6163,\n",
       "         'den': 6056,\n",
       "         'we': 5874,\n",
       "         'eine': 5868,\n",
       "         'mich': 5855,\n",
       "         'so': 5819,\n",
       "         'wie': 5721,\n",
       "         'war': 5699,\n",
       "         'on': 5517,\n",
       "         'ihr': 5499,\n",
       "         'und': 5485,\n",
       "         'be': 5475,\n",
       "         'and': 5468,\n",
       "         'she': 5420,\n",
       "         'with': 5297,\n",
       "         'not': 4924,\n",
       "         'will': 4805,\n",
       "         'like': 4712,\n",
       "         'at': 4646,\n",
       "         'haben': 4533,\n",
       "         'kann': 4501,\n",
       "         'know': 4334,\n",
       "         'want': 4329,\n",
       "         'maria': 4241,\n",
       "         'einen': 4225,\n",
       "         'von': 4194,\n",
       "         'sind': 4108,\n",
       "         'dem': 4017,\n",
       "         'fur': 3976,\n",
       "         'can': 3950,\n",
       "         'als': 3854,\n",
       "         'did': 3804,\n",
       "         'sein': 3613,\n",
       "         'has': 3607,\n",
       "         'how': 3601,\n",
       "         'bin': 3598,\n",
       "         'him': 3594,\n",
       "         'dich': 3569,\n",
       "         'hast': 3539,\n",
       "         'um': 3435,\n",
       "         'noch': 3433,\n",
       "         'dir': 3404,\n",
       "         'her': 3317,\n",
       "         'as': 3278,\n",
       "         'about': 3253,\n",
       "         'hatte': 3197,\n",
       "         'go': 3187,\n",
       "         'wurde': 3174,\n",
       "         'its': 3166,\n",
       "         'cant': 3165,\n",
       "         'aus': 3141,\n",
       "         'were': 3040,\n",
       "         'all': 3026,\n",
       "         'there': 3005,\n",
       "         'uns': 2990,\n",
       "         'time': 2979,\n",
       "         'nach': 2969,\n",
       "         'am': 2946,\n",
       "         'wird': 2871,\n",
       "         'etwas': 2864,\n",
       "         'very': 2858,\n",
       "         'had': 2851,\n",
       "         'think': 2843,\n",
       "         'sehr': 2836,\n",
       "         'up': 2810,\n",
       "         'didnt': 2796,\n",
       "         'hier': 2795,\n",
       "         'wenn': 2794,\n",
       "         'here': 2759,\n",
       "         'man': 2681,\n",
       "         'if': 2665,\n",
       "         'they': 2610,\n",
       "         'meine': 2560,\n",
       "         'get': 2558,\n",
       "         'out': 2531,\n",
       "         'from': 2427,\n",
       "         'schon': 2415,\n",
       "         'when': 2406,\n",
       "         'toms': 2391,\n",
       "         'no': 2340,\n",
       "         'mein': 2326,\n",
       "         'why': 2294,\n",
       "         'would': 2279,\n",
       "         'youre': 2238,\n",
       "         'ihn': 2223,\n",
       "         'by': 2166,\n",
       "         'mehr': 2128,\n",
       "         'bitte': 2111,\n",
       "         'one': 2108,\n",
       "         'been': 2071,\n",
       "         'vor': 2045,\n",
       "         'warum': 2025,\n",
       "         'please': 2025,\n",
       "         'good': 2021,\n",
       "         'just': 2009,\n",
       "         'bist': 1975,\n",
       "         'wei': 1971,\n",
       "         'help': 1933,\n",
       "         'gehen': 1922,\n",
       "         'keine': 1903,\n",
       "         'boston': 1889,\n",
       "         'werden': 1887,\n",
       "         'nichts': 1871,\n",
       "         'doesnt': 1867,\n",
       "         'going': 1866,\n",
       "         'tun': 1851,\n",
       "         'come': 1837,\n",
       "         'seine': 1827,\n",
       "         'see': 1824,\n",
       "         'diese': 1820,\n",
       "         'tell': 1808,\n",
       "         'immer': 1804,\n",
       "         'ill': 1801,\n",
       "         'need': 1799,\n",
       "         'gut': 1784,\n",
       "         'viel': 1765,\n",
       "         'ihm': 1758,\n",
       "         'never': 1746,\n",
       "         'us': 1743,\n",
       "         'nur': 1736,\n",
       "         'than': 1724,\n",
       "         'but': 1723,\n",
       "         'where': 1715,\n",
       "         'zum': 1707,\n",
       "         'werde': 1703,\n",
       "         'konnen': 1703,\n",
       "         'dieses': 1702,\n",
       "         'muss': 1701,\n",
       "         'could': 1696,\n",
       "         'who': 1685,\n",
       "         'should': 1646,\n",
       "         'wo': 1644,\n",
       "         'mochte': 1641,\n",
       "         'more': 1640,\n",
       "         'nie': 1622,\n",
       "         'zeit': 1621,\n",
       "         'uber': 1621,\n",
       "         'euch': 1610,\n",
       "         'take': 1600,\n",
       "         'konnte': 1579,\n",
       "         'now': 1565,\n",
       "         'sagen': 1557,\n",
       "         'heute': 1549,\n",
       "         'got': 1541,\n",
       "         'bei': 1500,\n",
       "         'much': 1499,\n",
       "         'machen': 1484,\n",
       "         'mal': 1458,\n",
       "         'ive': 1448,\n",
       "         'einem': 1433,\n",
       "         'said': 1432,\n",
       "         'some': 1428,\n",
       "         'something': 1421,\n",
       "         'too': 1404,\n",
       "         'deine': 1396,\n",
       "         'money': 1386,\n",
       "         'french': 1366,\n",
       "         'really': 1352,\n",
       "         'make': 1352,\n",
       "         'ihre': 1352,\n",
       "         'any': 1347,\n",
       "         'alle': 1346,\n",
       "         'da': 1330,\n",
       "         'car': 1329,\n",
       "         'alles': 1326,\n",
       "         'home': 1308,\n",
       "         'back': 1307,\n",
       "         'work': 1306,\n",
       "         'aber': 1305,\n",
       "         'well': 1302,\n",
       "         'thats': 1296,\n",
       "         'day': 1296,\n",
       "         'ihnen': 1294,\n",
       "         'geld': 1290,\n",
       "         'problem': 1284,\n",
       "         'anything': 1282,\n",
       "         'kannst': 1278,\n",
       "         'oder': 1264,\n",
       "         'dieser': 1259,\n",
       "         'viele': 1252,\n",
       "         'lot': 1249,\n",
       "         'right': 1242,\n",
       "         'jetzt': 1230,\n",
       "         'gerne': 1230,\n",
       "         'book': 1230,\n",
       "         'people': 1228,\n",
       "         'morgen': 1223,\n",
       "         'kein': 1222,\n",
       "         'gerade': 1213,\n",
       "         'einer': 1212,\n",
       "         'say': 1207,\n",
       "         'last': 1205,\n",
       "         'new': 1201,\n",
       "         'many': 1188,\n",
       "         'told': 1182,\n",
       "         'des': 1159,\n",
       "         'long': 1156,\n",
       "         'buch': 1137,\n",
       "         'hes': 1136,\n",
       "         'room': 1136,\n",
       "         'sagte': 1131,\n",
       "         'three': 1129,\n",
       "         'gibt': 1121,\n",
       "         'gesagt': 1120,\n",
       "         'house': 1117,\n",
       "         'franzosisch': 1112,\n",
       "         'does': 1106,\n",
       "         'drei': 1104,\n",
       "         'our': 1085,\n",
       "         'still': 1083,\n",
       "         'went': 1079,\n",
       "         'only': 1077,\n",
       "         'isnt': 1072,\n",
       "         'better': 1071,\n",
       "         'id': 1065,\n",
       "         'hause': 1055,\n",
       "         'always': 1053,\n",
       "         'wollte': 1053,\n",
       "         'made': 1049,\n",
       "         'give': 1048,\n",
       "         'arbeit': 1045,\n",
       "         'way': 1044,\n",
       "         'sehen': 1043,\n",
       "         'school': 1042,\n",
       "         'einmal': 1040,\n",
       "         'today': 1039,\n",
       "         'wer': 1030,\n",
       "         'speak': 1022,\n",
       "         'helfen': 1020,\n",
       "         'must': 1016,\n",
       "         'let': 1013,\n",
       "         'mussen': 1012,\n",
       "         'wirklich': 1004,\n",
       "         'put': 999,\n",
       "         'leben': 998,\n",
       "         'into': 998,\n",
       "         'off': 996,\n",
       "         'seinen': 985,\n",
       "         'dein': 982,\n",
       "         'before': 982,\n",
       "         'look': 971,\n",
       "         'thought': 967,\n",
       "         'sprechen': 965,\n",
       "         'haus': 963,\n",
       "         'old': 962,\n",
       "         'kommen': 960,\n",
       "         'zur': 955,\n",
       "         'ever': 953,\n",
       "         'again': 950,\n",
       "         'waren': 948,\n",
       "         'over': 946,\n",
       "         'talk': 945,\n",
       "         'asked': 941,\n",
       "         'wann': 940,\n",
       "         'ware': 935,\n",
       "         'two': 934,\n",
       "         'tag': 933,\n",
       "         'essen': 931,\n",
       "         'gesehen': 925,\n",
       "         'whats': 925,\n",
       "         'wissen': 917,\n",
       "         'glaube': 913,\n",
       "         'geht': 912,\n",
       "         'wieder': 908,\n",
       "         'love': 902,\n",
       "         'down': 901,\n",
       "         'eat': 900,\n",
       "         'habt': 899,\n",
       "         'leave': 887,\n",
       "         'lange': 886,\n",
       "         'lassen': 883,\n",
       "         'years': 882,\n",
       "         'vater': 882,\n",
       "         'mag': 881,\n",
       "         'besser': 881,\n",
       "         'other': 880,\n",
       "         'every': 873,\n",
       "         'gestern': 872,\n",
       "         'tomorrow': 870,\n",
       "         'meinen': 869,\n",
       "         'macht': 868,\n",
       "         'einfach': 863,\n",
       "         'auto': 856,\n",
       "         'or': 847,\n",
       "         'everything': 846,\n",
       "         'bis': 840,\n",
       "         'left': 839,\n",
       "         'sure': 839,\n",
       "         'father': 837,\n",
       "         'job': 834,\n",
       "         'kind': 824,\n",
       "         'after': 824,\n",
       "         'gemacht': 820,\n",
       "         'little': 817,\n",
       "         'find': 815,\n",
       "         'kommt': 814,\n",
       "         'willst': 808,\n",
       "         'lets': 806,\n",
       "         'schule': 806,\n",
       "         'mann': 801,\n",
       "         'wont': 801,\n",
       "         'auch': 800,\n",
       "         'ab': 797,\n",
       "         'wanted': 791,\n",
       "         'may': 790,\n",
       "         'keinen': 787,\n",
       "         'ins': 782,\n",
       "         'ob': 780,\n",
       "         'doch': 777,\n",
       "         'nothing': 772,\n",
       "         'next': 768,\n",
       "         'fast': 764,\n",
       "         'read': 763,\n",
       "         'zwei': 761,\n",
       "         'ging': 759,\n",
       "         'zimmer': 759,\n",
       "         'ask': 755,\n",
       "         'dog': 755,\n",
       "         'believe': 754,\n",
       "         'took': 751,\n",
       "         'name': 749,\n",
       "         'yesterday': 747,\n",
       "         'kam': 746,\n",
       "         'musst': 745,\n",
       "         'night': 736,\n",
       "         'ohne': 735,\n",
       "         'kinder': 735,\n",
       "         'bus': 734,\n",
       "         'uhr': 733,\n",
       "         'diesem': 728,\n",
       "         'stay': 724,\n",
       "         'doing': 724,\n",
       "         'mutter': 721,\n",
       "         'without': 720,\n",
       "         'getan': 718,\n",
       "         'hund': 715,\n",
       "         'sicher': 712,\n",
       "         'english': 709,\n",
       "         'children': 706,\n",
       "         'these': 704,\n",
       "         'frage': 704,\n",
       "         'marys': 704,\n",
       "         'wrong': 702,\n",
       "         'away': 695,\n",
       "         'couldnt': 693,\n",
       "         'buy': 689,\n",
       "         'first': 688,\n",
       "         'sollte': 688,\n",
       "         'saw': 687,\n",
       "         'their': 686,\n",
       "         'door': 681,\n",
       "         'done': 679,\n",
       "         'came': 677,\n",
       "         'keep': 676,\n",
       "         'seinem': 675,\n",
       "         'weit': 672,\n",
       "         'live': 670,\n",
       "         'theres': 668,\n",
       "         'diesen': 668,\n",
       "         'mother': 666,\n",
       "         'stop': 665,\n",
       "         'japan': 665,\n",
       "         'friends': 662,\n",
       "         'sei': 660,\n",
       "         'sollen': 659,\n",
       "         'seiner': 658,\n",
       "         'play': 657,\n",
       "         'them': 653,\n",
       "         'gehort': 651,\n",
       "         'even': 650,\n",
       "         'niemand': 650,\n",
       "         'happy': 648,\n",
       "         'plan': 647,\n",
       "         'life': 639,\n",
       "         'used': 638,\n",
       "         'call': 637,\n",
       "         'zuruck': 637,\n",
       "         'happened': 636,\n",
       "         'alone': 633,\n",
       "         'weg': 629,\n",
       "         'late': 628,\n",
       "         'dort': 625,\n",
       "         'sagt': 624,\n",
       "         'lost': 618,\n",
       "         'lass': 616,\n",
       "         'heard': 616,\n",
       "         'paar': 616,\n",
       "         'hilfe': 615,\n",
       "         'soon': 615,\n",
       "         'enough': 613,\n",
       "         'allein': 611,\n",
       "         'wusste': 608,\n",
       "         'lernen': 608,\n",
       "         'meinem': 607,\n",
       "         'jeden': 604,\n",
       "         'understand': 604,\n",
       "         'ganz': 600,\n",
       "         'wollen': 600,\n",
       "         'hand': 600,\n",
       "         'gern': 598,\n",
       "         'meiner': 598,\n",
       "         'ihren': 597,\n",
       "         'looking': 596,\n",
       "         'hard': 595,\n",
       "         'sah': 594,\n",
       "         'party': 594,\n",
       "         'found': 590,\n",
       "         'geben': 590,\n",
       "         'oft': 590,\n",
       "         'things': 590,\n",
       "         'solltest': 589,\n",
       "         'already': 589,\n",
       "         'feel': 587,\n",
       "         'soll': 587,\n",
       "         'selbst': 586,\n",
       "         'dachte': 586,\n",
       "         'wants': 581,\n",
       "         'seit': 579,\n",
       "         'denke': 577,\n",
       "         'often': 568,\n",
       "         'jemand': 567,\n",
       "         'try': 566,\n",
       "         'wasnt': 564,\n",
       "         'fragen': 563,\n",
       "         'bad': 562,\n",
       "         'gab': 562,\n",
       "         'each': 562,\n",
       "         'knows': 559,\n",
       "         'gave': 558,\n",
       "         'yet': 557,\n",
       "         'frau': 557,\n",
       "         'remember': 556,\n",
       "         'hatten': 556,\n",
       "         'damit': 554,\n",
       "         'sollten': 554,\n",
       "         'vielleicht': 553,\n",
       "         'wirst': 551,\n",
       "         'thing': 543,\n",
       "         'hope': 541,\n",
       "         'busy': 540,\n",
       "         'tur': 540,\n",
       "         'bought': 538,\n",
       "         'seid': 537,\n",
       "         'recht': 537,\n",
       "         'freund': 537,\n",
       "         'tut': 535,\n",
       "         'madchen': 533,\n",
       "         'morning': 531,\n",
       "         'seen': 531,\n",
       "         'englisch': 530,\n",
       "         'davon': 528,\n",
       "         'passiert': 528,\n",
       "         'week': 528,\n",
       "         'hear': 523,\n",
       "         'question': 523,\n",
       "         'bleiben': 520,\n",
       "         'deinen': 520,\n",
       "         'married': 519,\n",
       "         'true': 517,\n",
       "         'train': 517,\n",
       "         'answer': 516,\n",
       "         'friend': 516,\n",
       "         'idea': 516,\n",
       "         'youve': 514,\n",
       "         'spielen': 514,\n",
       "         'reden': 513,\n",
       "         'water': 513,\n",
       "         'leute': 513,\n",
       "         'around': 510,\n",
       "         'year': 510,\n",
       "         'wurden': 510,\n",
       "         'glauben': 509,\n",
       "         'wait': 508,\n",
       "         'best': 508,\n",
       "         'himself': 504,\n",
       "         'woche': 504,\n",
       "         'wish': 502,\n",
       "         'schnell': 501,\n",
       "         'because': 500,\n",
       "         'scheint': 498,\n",
       "         'fahren': 497,\n",
       "         'gute': 497,\n",
       "         'mind': 496,\n",
       "         'teacher': 496,\n",
       "         'few': 494,\n",
       "         'daruber': 493,\n",
       "         'same': 493,\n",
       "         'knew': 491,\n",
       "         'nacht': 491,\n",
       "         'jahre': 489,\n",
       "         'while': 489,\n",
       "         'cold': 487,\n",
       "         'else': 487,\n",
       "         'fehler': 486,\n",
       "         'warten': 484,\n",
       "         'bruder': 483,\n",
       "         'lie': 480,\n",
       "         'use': 479,\n",
       "         'able': 478,\n",
       "         'lesen': 474,\n",
       "         'menschen': 473,\n",
       "         'such': 472,\n",
       "         'open': 471,\n",
       "         'jahr': 471,\n",
       "         'anyone': 469,\n",
       "         'most': 469,\n",
       "         'stadt': 468,\n",
       "         'unter': 468,\n",
       "         'sieht': 468,\n",
       "         'likes': 466,\n",
       "         'lasst': 465,\n",
       "         'kaufen': 464,\n",
       "         'genug': 464,\n",
       "         'afraid': 463,\n",
       "         'zusammen': 463,\n",
       "         'coffee': 463,\n",
       "         'wouldnt': 463,\n",
       "         'anymore': 463,\n",
       "         'beim': 462,\n",
       "         'gegangen': 460,\n",
       "         'durch': 460,\n",
       "         'yourself': 460,\n",
       "         'spricht': 459,\n",
       "         'zug': 459,\n",
       "         'abend': 458,\n",
       "         'everyone': 458,\n",
       "         'horen': 457,\n",
       "         'konnten': 457,\n",
       "         'alt': 455,\n",
       "         'schwer': 455,\n",
       "         'someone': 455,\n",
       "         'watch': 452,\n",
       "         'drink': 452,\n",
       "         'being': 452,\n",
       "         'spat': 450,\n",
       "         'person': 450,\n",
       "         'big': 449,\n",
       "         'havent': 448,\n",
       "         'beautiful': 447,\n",
       "         'another': 446,\n",
       "         'letter': 446,\n",
       "         'jahren': 445,\n",
       "         'wasser': 444,\n",
       "         'days': 444,\n",
       "         'youll': 442,\n",
       "         'beschaftigt': 441,\n",
       "         'looked': 441,\n",
       "         'stand': 440,\n",
       "         'gekauft': 440,\n",
       "         'books': 440,\n",
       "         'wohl': 439,\n",
       "         'letzte': 439,\n",
       "         'almost': 437,\n",
       "         'shes': 436,\n",
       "         'freunde': 433,\n",
       "         'bed': 432,\n",
       "         'park': 432,\n",
       "         'seems': 429,\n",
       "         'looks': 429,\n",
       "         'gehe': 428,\n",
       "         'hattest': 428,\n",
       "         'write': 427,\n",
       "         'once': 427,\n",
       "         'meeting': 427,\n",
       "         'girl': 426,\n",
       "         'bald': 424,\n",
       "         'fertig': 424,\n",
       "         'learn': 424,\n",
       "         'brief': 423,\n",
       "         'tried': 422,\n",
       "         'gekommen': 421,\n",
       "         'both': 421,\n",
       "         'tennis': 421,\n",
       "         'chance': 421,\n",
       "         'wahr': 420,\n",
       "         'brother': 420,\n",
       "         'kaffee': 419,\n",
       "         'darf': 417,\n",
       "         'show': 415,\n",
       "         'finden': 414,\n",
       "         'glucklich': 413,\n",
       "         'bekommen': 411,\n",
       "         'getting': 411,\n",
       "         'boy': 408,\n",
       "         'parents': 408,\n",
       "         'food': 407,\n",
       "         'eltern': 406,\n",
       "         'hoffe': 405,\n",
       "         'says': 405,\n",
       "         'which': 405,\n",
       "         'might': 404,\n",
       "         'place': 404,\n",
       "         'hotel': 404,\n",
       "         'matter': 403,\n",
       "         'talking': 399,\n",
       "         'pay': 398,\n",
       "         'met': 398,\n",
       "         'tired': 398,\n",
       "         'treffen': 398,\n",
       "         'bucher': 397,\n",
       "         'unsere': 397,\n",
       "         'gefunden': 396,\n",
       "         'walk': 396,\n",
       "         'arent': 396,\n",
       "         'care': 395,\n",
       "         'turn': 395,\n",
       "         'world': 395,\n",
       "         'bett': 394,\n",
       "         'since': 393,\n",
       "         'daran': 392,\n",
       "         'happen': 392,\n",
       "         'lehrer': 391,\n",
       "         'nahm': 391,\n",
       "         'vergessen': 389,\n",
       "         'young': 386,\n",
       "         'sorry': 385,\n",
       "         'ihrer': 385,\n",
       "         'jeder': 385,\n",
       "         'ganze': 384,\n",
       "         'gefallt': 382,\n",
       "         'family': 381,\n",
       "         'anderen': 381,\n",
       "         'dazu': 380,\n",
       "         'weil': 380,\n",
       "         'junge': 379,\n",
       "         'genau': 379,\n",
       "         'doctor': 379,\n",
       "         'sleep': 378,\n",
       "         'waiting': 378,\n",
       "         'hours': 378,\n",
       "         'study': 377,\n",
       "         'dann': 376,\n",
       "         'denn': 376,\n",
       "         'accident': 376,\n",
       "         'augen': 375,\n",
       "         'vom': 374,\n",
       "         'liebe': 374,\n",
       "         'together': 373,\n",
       "         'baby': 373,\n",
       "         'ago': 373,\n",
       "         'brauchen': 372,\n",
       "         'andere': 371,\n",
       "         'schwester': 370,\n",
       "         'gewesen': 369,\n",
       "         'truth': 369,\n",
       "         'je': 369,\n",
       "         'angst': 367,\n",
       "         'great': 367,\n",
       "         'own': 367,\n",
       "         'ganzen': 366,\n",
       "         'station': 366,\n",
       "         'brauche': 365,\n",
       "         'gefallen': 364,\n",
       "         'einige': 364,\n",
       "         'mochtest': 364,\n",
       "         'until': 363,\n",
       "         'gar': 362,\n",
       "         'darauf': 362,\n",
       "         'verloren': 361,\n",
       "         'ten': 361,\n",
       "         'nehmen': 359,\n",
       "         'fragte': 358,\n",
       "         'meet': 355,\n",
       "         'schwimmen': 354,\n",
       "         'welt': 354,\n",
       "         'youd': 354,\n",
       "         'wunschte': 354,\n",
       "         'minutes': 354,\n",
       "         'died': 353,\n",
       "         'zehn': 353,\n",
       "         'early': 352,\n",
       "         'started': 352,\n",
       "         'alter': 352,\n",
       "         'uberhaupt': 352,\n",
       "         'quite': 351,\n",
       "         'phone': 351,\n",
       "         'geschichte': 351,\n",
       "         'sister': 350,\n",
       "         'arbeiten': 349,\n",
       "         'eines': 348,\n",
       "         'wahrheit': 347,\n",
       "         'easy': 346,\n",
       "         'schreiben': 345,\n",
       "         'wagen': 345,\n",
       "         'difficult': 345,\n",
       "         'bat': 344,\n",
       "         'mude': 343,\n",
       "         'favorite': 343,\n",
       "         'steht': 342,\n",
       "         'thank': 342,\n",
       "         'called': 341,\n",
       "         'groe': 341,\n",
       "         'computer': 341,\n",
       "         'coming': 340,\n",
       "         'moment': 340,\n",
       "         'lives': 340,\n",
       "         'bring': 339,\n",
       "         'fruh': 339,\n",
       "         'reading': 339,\n",
       "         'important': 339,\n",
       "         'tv': 338,\n",
       "         'worden': 338,\n",
       "         'fruher': 338,\n",
       "         'police': 337,\n",
       "         'marias': 337,\n",
       "         'kaum': 336,\n",
       "         'story': 336,\n",
       "         'trying': 335,\n",
       "         'ziemlich': 335,\n",
       "         'namen': 334,\n",
       "         'longer': 332,\n",
       "         'dies': 331,\n",
       "         'table': 331,\n",
       "         'those': 331,\n",
       "         'familie': 331,\n",
       "         'nobody': 330,\n",
       "         'musste': 330,\n",
       "         'strae': 330,\n",
       "         'usually': 329,\n",
       "         'nice': 327,\n",
       "         'neue': 327,\n",
       "         'working': 326,\n",
       "         'gro': 325,\n",
       "         'konntest': 325,\n",
       "         'gegessen': 324,\n",
       "         'deiner': 324,\n",
       "         'students': 323,\n",
       "         'wurdest': 323,\n",
       "         'beide': 322,\n",
       "         'stunden': 322,\n",
       "         'danke': 321,\n",
       "         'fun': 321,\n",
       "         'unser': 321,\n",
       "         'change': 320,\n",
       "         'office': 320,\n",
       "         'arzt': 319,\n",
       "         'meinung': 318,\n",
       "         'kenne': 317,\n",
       "         'lang': 317,\n",
       "         'times': 317,\n",
       "         'fenster': 317,\n",
       "         'letzten': 316,\n",
       "         'gleich': 315,\n",
       "         'leicht': 315,\n",
       "         'city': 315,\n",
       "         'rain': 314,\n",
       "         'japanese': 314,\n",
       "         'needs': 313,\n",
       "         'cat': 313,\n",
       "         'son': 313,\n",
       "         'cannot': 313,\n",
       "         'bank': 313,\n",
       "         'start': 312,\n",
       "         'eyes': 312,\n",
       "         'shouldnt': 312,\n",
       "         'playing': 312,\n",
       "         'dafur': 311,\n",
       "         'horte': 310,\n",
       "         'questions': 310,\n",
       "         'wife': 310,\n",
       "         'restaurant': 309,\n",
       "         'neuen': 308,\n",
       "         'sohn': 308,\n",
       "         'sorgen': 307,\n",
       "         'fell': 306,\n",
       "         'swim': 306,\n",
       "         'katze': 306,\n",
       "         'lieber': 306,\n",
       "         'bringen': 305,\n",
       "         'dollar': 304,\n",
       "         'warst': 304,\n",
       "         'eure': 304,\n",
       "         'gegen': 303,\n",
       "         'minuten': 303,\n",
       "         'forget': 302,\n",
       "         'myself': 301,\n",
       "         'braucht': 300,\n",
       "         'dinner': 300,\n",
       "         'land': 300,\n",
       "         'far': 299,\n",
       "         'drive': 299,\n",
       "         'hurt': 299,\n",
       "         'tisch': 299,\n",
       "         'hair': 299,\n",
       "         'picture': 299,\n",
       "         'halten': 298,\n",
       "         'leid': 297,\n",
       "         'tokyo': 297,\n",
       "         'word': 297,\n",
       "         'ran': 296,\n",
       "         'ihrem': 296,\n",
       "         'five': 296,\n",
       "         'john': 296,\n",
       "         'film': 294,\n",
       "         'mistake': 294,\n",
       "         'welche': 292,\n",
       "         'verstehen': 292,\n",
       "         'getroffen': 291,\n",
       "         'machte': 291,\n",
       "         'liegt': 290,\n",
       "         'bisschen': 290,\n",
       "         'funf': 289,\n",
       "         'number': 289,\n",
       "         'pretty': 289,\n",
       "         'country': 289,\n",
       "         'student': 288,\n",
       "         'nachsten': 288,\n",
       "         'weve': 287,\n",
       "         'wahrend': 287,\n",
       "         'mean': 286,\n",
       "         'finished': 286,\n",
       "         'under': 286,\n",
       "         'london': 285,\n",
       "         'through': 285,\n",
       "         'bevor': 285,\n",
       "         'mine': 284,\n",
       "         'gib': 284,\n",
       "         'window': 284,\n",
       "         'erst': 284,\n",
       "         'geschrieben': 283,\n",
       "         'angry': 282,\n",
       "         'wegen': 282,\n",
       "         'sick': 281,\n",
       "         'polizei': 281,\n",
       "         'fire': 280,\n",
       "         'hin': 280,\n",
       "         'krank': 280,\n",
       "         'guter': 280,\n",
       "         'child': 280,\n",
       "         'turned': 280,\n",
       "         'tage': 280,\n",
       "         'mach': 279,\n",
       "         'fish': 279,\n",
       "         'theyre': 279,\n",
       "         'living': 278,\n",
       "         'news': 277,\n",
       "         'wearing': 277,\n",
       "         'bahnhof': 277,\n",
       "         'pass': 276,\n",
       "         'half': 276,\n",
       "         'wort': 276,\n",
       "         'shouldve': 276,\n",
       "         'los': 274,\n",
       "         'gone': 274,\n",
       "         'decided': 274,\n",
       "         'trinken': 273,\n",
       "         'small': 273,\n",
       "         'trouble': 272,\n",
       "         'movie': 271,\n",
       "         'music': 270,\n",
       "         'spa': 269,\n",
       "         'versuchte': 269,\n",
       "         'brauchst': 269,\n",
       "         'seem': 268,\n",
       "         'run': 267,\n",
       "         'ende': 267,\n",
       "         'breakfast': 267,\n",
       "         'isst': 266,\n",
       "         'advice': 266,\n",
       "         'situation': 266,\n",
       "         'neues': 265,\n",
       "         'makes': 265,\n",
       "         'arm': 264,\n",
       "         'ja': 264,\n",
       "         'fahrt': 264,\n",
       "         'rauchen': 264,\n",
       "         'everybody': 264,\n",
       "         'taxi': 264,\n",
       "         'sag': 263,\n",
       "         'konnt': 263,\n",
       "         'lunch': 263,\n",
       "         'arrived': 263,\n",
       "         'alte': 263,\n",
       "         'walked': 261,\n",
       "         'month': 261,\n",
       "         'groer': 259,\n",
       "         'having': 259,\n",
       "         'versuchen': 258,\n",
       "         'fall': 258,\n",
       "         'town': 258,\n",
       "         'sofort': 258,\n",
       "         'komme': 256,\n",
       "         'killed': 256,\n",
       "         'caught': 256,\n",
       "         'wenig': 255,\n",
       "         'menge': 255,\n",
       "         'lied': 254,\n",
       "         'weiter': 254,\n",
       "         'broke': 253,\n",
       "         'sonst': 253,\n",
       "         'wohnt': 253,\n",
       "         'gewohnlich': 253,\n",
       "         'ready': 252,\n",
       "         'worry': 252,\n",
       "         'winter': 252,\n",
       "         'groen': 252,\n",
       "         'schlafen': 251,\n",
       "         'fahrrad': 251,\n",
       "         'mochten': 251,\n",
       "         'hot': 249,\n",
       "         'hungry': 249,\n",
       "         'street': 249,\n",
       "         'idee': 248,\n",
       "         'interested': 248,\n",
       "         'antwort': 248,\n",
       "         'miss': 247,\n",
       "         'goes': 247,\n",
       "         'nahe': 246,\n",
       "         'shoes': 246,\n",
       "         'versucht': 245,\n",
       "         'eating': 245,\n",
       "         'tat': 245,\n",
       "         'verlassen': 244,\n",
       "         'stupid': 244,\n",
       "         'gelesen': 244,\n",
       "         'spielt': 244,\n",
       "         'interesting': 244,\n",
       "         'glaubst': 243,\n",
       "         'woman': 243,\n",
       "         'river': 243,\n",
       "         'drauen': 242,\n",
       "         'high': 242,\n",
       "         'unfall': 242,\n",
       "         'stelle': 241,\n",
       "         'light': 241,\n",
       "         'sat': 240,\n",
       "         'grund': 240,\n",
       "         'older': 240,\n",
       "         'geworden': 240,\n",
       "         'komm': 239,\n",
       "         'arbeitet': 239,\n",
       "         'glad': 238,\n",
       "         'hands': 237,\n",
       "         'later': 237,\n",
       "         'einzige': 237,\n",
       "         'spater': 236,\n",
       "         'death': 236,\n",
       "         'exactly': 236,\n",
       "         'dollars': 236,\n",
       "         'hielt': 234,\n",
       "         'large': 234,\n",
       "         'besuchen': 234,\n",
       "         'tochter': 234,\n",
       "         'then': 233,\n",
       "         'stimmt': 233,\n",
       "         'jungen': 233,\n",
       "         'glaubt': 233,\n",
       "         'behind': 233,\n",
       "         'surprised': 233,\n",
       "         'become': 233,\n",
       "         'gegeben': 232,\n",
       "         'smoking': 232,\n",
       "         'known': 232,\n",
       "         'clothes': 232,\n",
       "         ...})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:11.222943Z",
     "start_time": "2024-12-01T12:59:11.192780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_word_freq = 3\n",
    "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "word_map = {k: v+1 for v, k in enumerate(words)}\n",
    "\n",
    "#adding special tokens\n",
    "word_map['<unk>'] = len(word_map) + 1\n",
    "word_map['<start>'] = len(word_map) + 1\n",
    "word_map['<end>'] = len(word_map) + 1\n",
    "word_map['<pad>'] = 0"
   ],
   "id": "36ae293b8bea4f38",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:11.270523Z",
     "start_time": "2024-12-01T12:59:11.225023Z"
    }
   },
   "cell_type": "code",
   "source": "len(word_map)",
   "id": "5bfba3be7a1a284c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16617"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:11.286561Z",
     "start_time": "2024-12-01T12:59:11.270523Z"
    }
   },
   "cell_type": "code",
   "source": "word_map['<unk>'], word_map['<start>'], word_map['<end>'], word_map['<pad>']",
   "id": "e0be444e14b063cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16614, 16615, 16616, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:11.381165Z",
     "start_time": "2024-12-01T12:59:11.286561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('word_map_corpus.json', 'w') as j:\n",
    "\tjson.dump(word_map, j)"
   ],
   "id": "e60c31970446127e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:11.396800Z",
     "start_time": "2024-12-01T12:59:11.381165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_enc_inp(words, word_map):\n",
    "    # Get the encoded words\n",
    "    enc_c = [word_map.get(word, word_map['<unk>']) for word in words]\n",
    "    # Truncate to max_len\n",
    "    enc_c = enc_c[:max_len]\n",
    "    # Pad to max_len\n",
    "    enc_c += [word_map['<pad>']] * (max_len - len(enc_c))\n",
    "    return enc_c\n",
    "\n",
    "def encode_dec_inp(words, word_map):\n",
    "    # Start with the start token\n",
    "    enc_c = [word_map['<start>']]\n",
    "    \n",
    "    # Encode the words, limiting the length to max_len - 2 to account for <start> and <end>\n",
    "    enc_c += [word_map.get(word, word_map['<unk>']) for word in words][:max_len - 2]\n",
    "    \n",
    "    # Add the end token if there is space\n",
    "    enc_c.append(word_map['<end>'])\n",
    "    \n",
    "    # Pad to max_len + 1 (to include the end token)\n",
    "    enc_c += [word_map['<pad>']] * (max_len - len(enc_c))\n",
    "    \n",
    "    return enc_c"
   ],
   "id": "c1b68eb3b26b1a98",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:15.130798Z",
     "start_time": "2024-12-01T12:59:11.397881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_len = 20\n",
    "#initialize an empty list to store encoded translate pairs\n",
    "# Check the lengths of the encoded pairs\n",
    "pairs_encoded = []\n",
    "for pair in clean_pairs:\n",
    "    english = encode_enc_inp(pair[0], word_map)\n",
    "    german = encode_dec_inp(pair[1], word_map)\n",
    "    pairs_encoded.append([english, german])"
   ],
   "id": "5800ea035ceed2f5",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:15.146272Z",
     "start_time": "2024-12-01T12:59:15.131474Z"
    }
   },
   "cell_type": "code",
   "source": "clean_pairs[1]",
   "id": "4b99dcacbda11930",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hi', 'gru gott'], dtype='<U370')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:15.194819Z",
     "start_time": "2024-12-01T12:59:15.150427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print a sample of the encoded pairs to verify\n",
    "print(\"Sample Encoded Pair:\")\n",
    "print(f\"English: {pairs_encoded[0][0]}\")\n",
    "print(f\"German: {pairs_encoded[0][1]}\")\n"
   ],
   "id": "24778437f61917f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Encoded Pair:\n",
      "English: [16614, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "German: [16615, 16614, 207, 16614, 16614, 5266, 16616, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:29.103590Z",
     "start_time": "2024-12-01T12:59:15.197012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#saving number coded wordmap\n",
    "f_name = 'pairs_encoded.json'\n",
    "with open(f_name, 'w') as p:\n",
    "\tjson.dump(pairs_encoded, p)"
   ],
   "id": "2b49bab8682eae1c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:29.119040Z",
     "start_time": "2024-12-01T12:59:29.106546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tensor_to_sentence(t, clean=False):\n",
    "\tq = t.detach().numpy()\n",
    "\tq_words = \" \".join([rev_word_map[v] for v in q])\n",
    "\tif clean:\n",
    "\t\tq_words = q_words.replace(\"<pad>\", \"\")\n",
    "\treturn q_words"
   ],
   "id": "b2a6df76057effd3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:29.165875Z",
     "start_time": "2024-12-01T12:59:29.122279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rev_word_map = {v: k for k, v in word_map.items()}  # Reverse mapping for decoding\n",
    "max_len = 20  # Max length for sequences, including <start> and <end>\n",
    "batch_size = 32  # Example batch size\n",
    "\n",
    "# Dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, word_map, max_len):\n",
    "        self.word_map = word_map\n",
    "        self.max_len = max_len\n",
    "        self.pairs = json.load(open('pairs_encoded.json'))  # Load your encoded pairs\n",
    "        self.dataset_size = len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        enc_inp = torch.LongTensor(self.pairs[index][0])  # Assuming pairs contain encoded inputs\n",
    "        dec = torch.LongTensor(self.pairs[index][1])\n",
    "        \n",
    "        dec_inp = dec[:-1]  # Input for decoder\n",
    "        dec_out = dec[1:]   # Output for decoder\n",
    "        \n",
    "        # Ensure all sequences are padded or truncated to max_len\n",
    "        enc_inp = enc_inp[:self.max_len]\n",
    "        dec_inp = dec_inp[:self.max_len + 1]  # Adjusted for the decoder input\n",
    "        dec_out = dec_out[:self.max_len]  # This should still be max_len\n",
    "\n",
    "        return enc_inp, dec_inp, dec_out\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "# Token Embedding class\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, pad_id):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_id)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize embedding weights uniformly within [-0.1, 0.1].\"\"\"\n",
    "        initrange = 0.1\n",
    "        self.token_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.token_embedding(x)\n",
    "\n",
    "# Positional Encoding class\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = self.pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)  # Get current sequence length\n",
    "        return self.pe[:, :seq_len].to(x.device)\n",
    "\n",
    "# Embeddings class\n",
    "# In the Embeddings class\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, word_map, embed_size, max_len):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(num_embeddings=len(word_map), embedding_dim=embed_size, padding_idx=word_map['<pad>'])\n",
    "        self.pos_embedding = PositionalEncoding(embed_size, max_len + 2)  # Including <start> and <end>\n",
    "        self.embed_size = embed_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        token_embed = self.token_embedding(x) * math.sqrt(self.embed_size)  # Scale embeddings\n",
    "        pos_embed = self.pos_embedding(x)[:, :x.size(1), :]\n",
    "        return token_embed + pos_embed\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, word_map, d_model=512, n_heads=8, num_encoder_layers=6, num_decoder_layers=6,\n",
    "                 dim_feedforward=2048, dropout=0.1, max_len=20):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_embedding = Embeddings(word_map=word_map, embed_size=d_model, max_len=max_len)\n",
    "        self.transformer = nn.Transformer(d_model=d_model,\n",
    "                                           nhead=n_heads,\n",
    "                                           num_encoder_layers=num_encoder_layers,\n",
    "                                           num_decoder_layers=num_decoder_layers,\n",
    "                                           dim_feedforward=dim_feedforward,\n",
    "                                           dropout=dropout,\n",
    "                                           batch_first=True)\n",
    "        self.project_vocab_layer = nn.Linear(d_model, len(word_map))  # Ensure vocab_size is correct\n",
    "\n",
    "    def forward(self, enc_input, dec_input):\n",
    "        enc_input = enc_input[:, :max_len]  # Truncate encoder input if necessary\n",
    "        dec_input = dec_input[:, :max_len + 1]  # Truncate decoder input if necessary\n",
    "\n",
    "        x_enc_embed = self.input_embedding(enc_input.long())  # (batch_size, enc_seq_len, d_model)\n",
    "\n",
    "        x_dec_embed = self.input_embedding(dec_input.long())  # (batch_size, dec_seq_len, d_model)\n",
    "\n",
    "        # Masks\n",
    "        src_key_padding_mask = (enc_input == word_map['<pad>']).to(enc_input.device)\n",
    "        tgt_key_padding_mask = (dec_input == word_map['<pad>']).to(enc_input.device)\n",
    "\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(dec_input.size(1)).to(enc_input.device)\n",
    "\n",
    "        # Forward pass through transformer\n",
    "        feature = self.transformer(src=x_enc_embed,\n",
    "                                   tgt=x_dec_embed,\n",
    "                                   src_key_padding_mask=src_key_padding_mask,\n",
    "                                   tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                   memory_key_padding_mask=src_key_padding_mask,\n",
    "                                   tgt_mask=tgt_mask)\n",
    "\n",
    "        logits = self.project_vocab_layer(feature)  # Shape should be (batch_size, max_len, vocab_size)\n",
    "        return logits"
   ],
   "id": "5123683f98460aee",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:31.434704Z",
     "start_time": "2024-12-01T12:59:29.165875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = TranslationDataset(word_map, max_len)\n",
    "train_data[:2]"
   ],
   "id": "c89bb9f8bbbeb13e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[16614,    17,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [16615, 16614,   207, 16614, 16614,  5266, 16616,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " tensor([[16614,    17,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " tensor([[16615, 16614, 16614, 16614, 16614, 16614,  5266, 16614, 16614, 16616,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:59:31.459796Z",
     "start_time": "2024-12-01T12:59:31.438395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_sentence(sentence, word_map, max_len):\n",
    "    # Convert words to their corresponding indices\n",
    "    encoded = [word_map.get(word, word_map['<unk>']) for word in sentence.split()]\n",
    "    \n",
    "    # Add <start> and <end> tokens\n",
    "    encoded = [word_map['<start>']] + encoded + [word_map['<end>']]\n",
    "    \n",
    "    # Pad or truncate to max_len\n",
    "    if len(encoded) < max_len:\n",
    "        encoded += [word_map['<pad>']] * (max_len - len(encoded))\n",
    "    else:\n",
    "        encoded = encoded[:max_len]\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "# Example usage\n",
    "sentence = \"hello world\"\n",
    "encoded_sentence = encode_sentence(sentence, word_map, max_len=20)\n",
    "print(encoded_sentence)"
   ],
   "id": "e59a51d3f0c33685",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16615, 16, 5175, 16616, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T13:05:13.230831Z",
     "start_time": "2024-12-01T12:59:31.463140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import optim\n",
    "\n",
    "# Define model parameters\n",
    "embed_size = 64  # Size of the embedding vectors\n",
    "hidden_size = 256  # Size of the hidden layers\n",
    "num_layers = 1  # Number of layers in the Transformer\n",
    "num_heads = 8  # Number of attention heads\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "# Initialize the model\n",
    "model = Transformer(word_map=word_map, n_heads=num_heads, dropout=dropout)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_map['<pad>'])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = TranslationDataset(word_map, max_len)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # Number of epochs to train\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for enc_inp, dec_inp, dec_out in data_loader:\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(enc_inp, dec_inp)  # Output shape should be (batch_size, max_len, vocab_size)\n",
    "\n",
    "        # Calculate loss\n",
    "        # Reshape output and dec_out for loss calculation\n",
    "        output_reshaped = output.view(-1, len(word_map))  # Shape: (batch_size * max_len, vocab_size)\n",
    "        dec_out_reshaped = dec_out.view(-1)  # Shape: (batch_size * max_len)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output_reshaped, dec_out_reshaped)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ],
   "id": "c992ff0854a41859",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smari\\PycharmProjects\\transfer_learning\\anomalib_env\\lib\\site-packages\\torch\\nn\\functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T13:05:13.237607Z",
     "start_time": "2024-12-01T13:05:13.236752Z"
    }
   },
   "cell_type": "code",
   "source": "#",
   "id": "595f19a7810664e1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
