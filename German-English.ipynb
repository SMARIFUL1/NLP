{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-29T19:36:41.146388Z",
     "start_time": "2024-11-29T19:36:27.711596Z"
    }
   },
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:36:41.162291Z",
     "start_time": "2024-11-29T19:36:41.146388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "max_len = 16\n",
    "num_heads = 8"
   ],
   "id": "ecfe6b5b18d06239",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:36:41.178093Z",
     "start_time": "2024-11-29T19:36:41.163313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ],
   "id": "9b2e5199ad2ca225",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:36:47.176913Z",
     "start_time": "2024-11-29T19:36:47.148656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "\tlines = doc.strip().split('\\n')\n",
    "\tpairs = [line.split('\\t') for line in  lines]\n",
    "\treturn pairs"
   ],
   "id": "c2355e2d254624d0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:36:48.264257Z",
     "start_time": "2024-11-29T19:36:48.250071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_pairs(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor pair in lines:\n",
    "\t\tclean_pair = list()\n",
    "\t\tfor line in pair:\n",
    "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "\t\t\tline = line.decode('UTF-8')\n",
    "\t\t\t# tokenize on white space\n",
    "\t\t\tline = line.split()\n",
    "\t\t\tline = [word.lower() for word in line]\n",
    "\t\t\t# remove punctuation from each token\n",
    "\t\t\tline = [word.translate(table) for word in line]\n",
    "\t\t\t# remove non-printable chars form each token\n",
    "\t\t\tline = [re_print.sub('', w) for w in line]\n",
    "\t\t\t# remove tokens with numbers in them\n",
    "\t\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t\t# store as string\n",
    "\t\t\tclean_pair.append(' '.join(line))\n",
    "\t\tcleaned.append(clean_pair)\n",
    "\treturn array(cleaned)"
   ],
   "id": "f9516ef4b1a34b91",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:36:49.284289Z",
     "start_time": "2024-11-29T19:36:49.268189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)"
   ],
   "id": "494be1c7b10d2e2b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:37:00.290046Z",
     "start_time": "2024-11-29T19:36:49.954740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filename = \"D:\\data\\deu.txt\"\n",
    "doc = load_doc(filename)\n",
    "pairs = to_pairs(doc)\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "save_clean_data(clean_pairs, 'english-german.pkl')\n",
    "for i in range(100):\n",
    "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ],
   "id": "8274a1609c3e93ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n",
      "[hi] => [hallo]\n",
      "[hi] => [gru gott]\n",
      "[run] => [lauf]\n",
      "[wow] => [potzdonner]\n",
      "[wow] => [donnerwetter]\n",
      "[fire] => [feuer]\n",
      "[help] => [hilfe]\n",
      "[help] => [zu hulf]\n",
      "[stop] => [stopp]\n",
      "[wait] => [warte]\n",
      "[hello] => [hallo]\n",
      "[i try] => [ich probiere es]\n",
      "[i won] => [ich hab gewonnen]\n",
      "[i won] => [ich habe gewonnen]\n",
      "[smile] => [lacheln]\n",
      "[cheers] => [zum wohl]\n",
      "[freeze] => [keine bewegung]\n",
      "[freeze] => [stehenbleiben]\n",
      "[got it] => [verstanden]\n",
      "[got it] => [einverstanden]\n",
      "[he ran] => [er rannte]\n",
      "[he ran] => [er lief]\n",
      "[hop in] => [mach mit]\n",
      "[hug me] => [druck mich]\n",
      "[hug me] => [nimm mich in den arm]\n",
      "[hug me] => [umarme mich]\n",
      "[i fell] => [ich fiel]\n",
      "[i fell] => [ich fiel hin]\n",
      "[i fell] => [ich sturzte]\n",
      "[i fell] => [ich bin hingefallen]\n",
      "[i fell] => [ich bin gesturzt]\n",
      "[i know] => [ich wei]\n",
      "[i lied] => [ich habe gelogen]\n",
      "[i lost] => [ich habe verloren]\n",
      "[im] => [ich bin jahre alt]\n",
      "[im] => [ich bin]\n",
      "[im ok] => [mir gehts gut]\n",
      "[im ok] => [es geht mir gut]\n",
      "[no way] => [unmoglich]\n",
      "[no way] => [das gibts doch nicht]\n",
      "[no way] => [ausgeschlossen]\n",
      "[no way] => [in keinster weise]\n",
      "[really] => [wirklich]\n",
      "[really] => [echt]\n",
      "[really] => [im ernst]\n",
      "[thanks] => [danke]\n",
      "[try it] => [versuchs]\n",
      "[why me] => [warum ich]\n",
      "[ask tom] => [frag tom]\n",
      "[ask tom] => [fragen sie tom]\n",
      "[ask tom] => [fragt tom]\n",
      "[be cool] => [entspann dich]\n",
      "[be fair] => [sei nicht ungerecht]\n",
      "[be fair] => [sei fair]\n",
      "[be nice] => [sei nett]\n",
      "[be nice] => [seien sie nett]\n",
      "[beat it] => [geh weg]\n",
      "[beat it] => [hau ab]\n",
      "[beat it] => [verschwinde]\n",
      "[beat it] => [verdufte]\n",
      "[beat it] => [mach dich fort]\n",
      "[beat it] => [zieh leine]\n",
      "[beat it] => [mach dich vom acker]\n",
      "[beat it] => [verzieh dich]\n",
      "[beat it] => [verkrumele dich]\n",
      "[beat it] => [troll dich]\n",
      "[beat it] => [zisch ab]\n",
      "[beat it] => [pack dich]\n",
      "[beat it] => [mach ne fliege]\n",
      "[beat it] => [schwirr ab]\n",
      "[beat it] => [mach die sause]\n",
      "[beat it] => [scher dich weg]\n",
      "[beat it] => [scher dich fort]\n",
      "[call me] => [ruf mich an]\n",
      "[come in] => [komm herein]\n",
      "[come in] => [herein]\n",
      "[come on] => [komm]\n",
      "[come on] => [kommt]\n",
      "[come on] => [mach schon]\n",
      "[come on] => [macht schon]\n",
      "[get out] => [raus]\n",
      "[go away] => [geh weg]\n",
      "[go away] => [hau ab]\n",
      "[go away] => [verschwinde]\n",
      "[go away] => [verdufte]\n",
      "[go away] => [mach dich fort]\n",
      "[go away] => [zieh leine]\n",
      "[go away] => [mach dich vom acker]\n",
      "[go away] => [verzieh dich]\n",
      "[go away] => [verkrumele dich]\n",
      "[go away] => [troll dich]\n",
      "[go away] => [zisch ab]\n",
      "[go away] => [pack dich]\n",
      "[go away] => [mach ne fliege]\n",
      "[go away] => [schwirr ab]\n",
      "[go away] => [mach die sause]\n",
      "[go away] => [scher dich weg]\n",
      "[go away] => [scher dich fort]\n",
      "[go away] => [geh weg]\n",
      "[go away] => [verpiss dich]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:37:00.337070Z",
     "start_time": "2024-11-29T19:37:00.290046Z"
    }
   },
   "cell_type": "code",
   "source": "len(clean_pairs)",
   "id": "a3616795eb367fd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152820"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:37:00.400024Z",
     "start_time": "2024-11-29T19:37:00.337070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in clean_pairs:\n",
    "\tif len(p) !=2:\n",
    "\t\tprint(len(p))"
   ],
   "id": "95cf2e8c6d71acd9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:37:02.282617Z",
     "start_time": "2024-11-29T19:37:00.400024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_freq = Counter()\n",
    "for pair in clean_pairs:\n",
    "\tword_freq.update(pair[0].split())\n",
    "\tword_freq.update(pair[1].split())\n",
    "word_freq"
   ],
   "id": "e09f4a6f94f61024",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'tom': 60434,\n",
       "         'ich': 40537,\n",
       "         'the': 37212,\n",
       "         'to': 34851,\n",
       "         'you': 34260,\n",
       "         'i': 32855,\n",
       "         'a': 24014,\n",
       "         'in': 21454,\n",
       "         'ist': 21346,\n",
       "         'nicht': 21081,\n",
       "         'sie': 19804,\n",
       "         'is': 18781,\n",
       "         'du': 17570,\n",
       "         'das': 17320,\n",
       "         'was': 16128,\n",
       "         'zu': 15622,\n",
       "         'die': 14282,\n",
       "         'es': 13890,\n",
       "         'er': 13412,\n",
       "         'he': 12141,\n",
       "         'of': 11448,\n",
       "         'der': 11165,\n",
       "         'it': 10381,\n",
       "         'that': 10358,\n",
       "         'do': 9481,\n",
       "         'have': 9247,\n",
       "         'this': 9100,\n",
       "         'hat': 9077,\n",
       "         'me': 8858,\n",
       "         'ein': 8730,\n",
       "         'dass': 8274,\n",
       "         'for': 7841,\n",
       "         'im': 7812,\n",
       "         'my': 7684,\n",
       "         'wir': 7604,\n",
       "         'habe': 7184,\n",
       "         'an': 7087,\n",
       "         'mary': 7063,\n",
       "         'mir': 6942,\n",
       "         'dont': 6851,\n",
       "         'auf': 6567,\n",
       "         'sich': 6529,\n",
       "         'your': 6405,\n",
       "         'mit': 6402,\n",
       "         'are': 6392,\n",
       "         'what': 6297,\n",
       "         'his': 6163,\n",
       "         'den': 6056,\n",
       "         'we': 5874,\n",
       "         'eine': 5868,\n",
       "         'mich': 5855,\n",
       "         'so': 5819,\n",
       "         'wie': 5721,\n",
       "         'war': 5699,\n",
       "         'on': 5517,\n",
       "         'ihr': 5499,\n",
       "         'und': 5485,\n",
       "         'be': 5475,\n",
       "         'and': 5468,\n",
       "         'she': 5420,\n",
       "         'with': 5297,\n",
       "         'not': 4924,\n",
       "         'will': 4805,\n",
       "         'like': 4712,\n",
       "         'at': 4646,\n",
       "         'haben': 4533,\n",
       "         'kann': 4501,\n",
       "         'know': 4334,\n",
       "         'want': 4329,\n",
       "         'maria': 4241,\n",
       "         'einen': 4225,\n",
       "         'von': 4194,\n",
       "         'sind': 4108,\n",
       "         'dem': 4017,\n",
       "         'fur': 3976,\n",
       "         'can': 3950,\n",
       "         'als': 3854,\n",
       "         'did': 3804,\n",
       "         'sein': 3613,\n",
       "         'has': 3607,\n",
       "         'how': 3601,\n",
       "         'bin': 3598,\n",
       "         'him': 3594,\n",
       "         'dich': 3569,\n",
       "         'hast': 3539,\n",
       "         'um': 3435,\n",
       "         'noch': 3433,\n",
       "         'dir': 3404,\n",
       "         'her': 3317,\n",
       "         'as': 3278,\n",
       "         'about': 3253,\n",
       "         'hatte': 3197,\n",
       "         'go': 3187,\n",
       "         'wurde': 3174,\n",
       "         'its': 3166,\n",
       "         'cant': 3165,\n",
       "         'aus': 3141,\n",
       "         'were': 3040,\n",
       "         'all': 3026,\n",
       "         'there': 3005,\n",
       "         'uns': 2990,\n",
       "         'time': 2979,\n",
       "         'nach': 2969,\n",
       "         'am': 2946,\n",
       "         'wird': 2871,\n",
       "         'etwas': 2864,\n",
       "         'very': 2858,\n",
       "         'had': 2851,\n",
       "         'think': 2843,\n",
       "         'sehr': 2836,\n",
       "         'up': 2810,\n",
       "         'didnt': 2796,\n",
       "         'hier': 2795,\n",
       "         'wenn': 2794,\n",
       "         'here': 2759,\n",
       "         'man': 2681,\n",
       "         'if': 2665,\n",
       "         'they': 2610,\n",
       "         'meine': 2560,\n",
       "         'get': 2558,\n",
       "         'out': 2531,\n",
       "         'from': 2427,\n",
       "         'schon': 2415,\n",
       "         'when': 2406,\n",
       "         'toms': 2391,\n",
       "         'no': 2340,\n",
       "         'mein': 2326,\n",
       "         'why': 2294,\n",
       "         'would': 2279,\n",
       "         'youre': 2238,\n",
       "         'ihn': 2223,\n",
       "         'by': 2166,\n",
       "         'mehr': 2128,\n",
       "         'bitte': 2111,\n",
       "         'one': 2108,\n",
       "         'been': 2071,\n",
       "         'vor': 2045,\n",
       "         'warum': 2025,\n",
       "         'please': 2025,\n",
       "         'good': 2021,\n",
       "         'just': 2009,\n",
       "         'bist': 1975,\n",
       "         'wei': 1971,\n",
       "         'help': 1933,\n",
       "         'gehen': 1922,\n",
       "         'keine': 1903,\n",
       "         'boston': 1889,\n",
       "         'werden': 1887,\n",
       "         'nichts': 1871,\n",
       "         'doesnt': 1867,\n",
       "         'going': 1866,\n",
       "         'tun': 1851,\n",
       "         'come': 1837,\n",
       "         'seine': 1827,\n",
       "         'see': 1824,\n",
       "         'diese': 1820,\n",
       "         'tell': 1808,\n",
       "         'immer': 1804,\n",
       "         'ill': 1801,\n",
       "         'need': 1799,\n",
       "         'gut': 1784,\n",
       "         'viel': 1765,\n",
       "         'ihm': 1758,\n",
       "         'never': 1746,\n",
       "         'us': 1743,\n",
       "         'nur': 1736,\n",
       "         'than': 1724,\n",
       "         'but': 1723,\n",
       "         'where': 1715,\n",
       "         'zum': 1707,\n",
       "         'werde': 1703,\n",
       "         'konnen': 1703,\n",
       "         'dieses': 1702,\n",
       "         'muss': 1701,\n",
       "         'could': 1696,\n",
       "         'who': 1685,\n",
       "         'should': 1646,\n",
       "         'wo': 1644,\n",
       "         'mochte': 1641,\n",
       "         'more': 1640,\n",
       "         'nie': 1622,\n",
       "         'zeit': 1621,\n",
       "         'uber': 1621,\n",
       "         'euch': 1610,\n",
       "         'take': 1600,\n",
       "         'konnte': 1579,\n",
       "         'now': 1565,\n",
       "         'sagen': 1557,\n",
       "         'heute': 1549,\n",
       "         'got': 1541,\n",
       "         'bei': 1500,\n",
       "         'much': 1499,\n",
       "         'machen': 1484,\n",
       "         'mal': 1458,\n",
       "         'ive': 1448,\n",
       "         'einem': 1433,\n",
       "         'said': 1432,\n",
       "         'some': 1428,\n",
       "         'something': 1421,\n",
       "         'too': 1404,\n",
       "         'deine': 1396,\n",
       "         'money': 1386,\n",
       "         'french': 1366,\n",
       "         'really': 1352,\n",
       "         'make': 1352,\n",
       "         'ihre': 1352,\n",
       "         'any': 1347,\n",
       "         'alle': 1346,\n",
       "         'da': 1330,\n",
       "         'car': 1329,\n",
       "         'alles': 1326,\n",
       "         'home': 1308,\n",
       "         'back': 1307,\n",
       "         'work': 1306,\n",
       "         'aber': 1305,\n",
       "         'well': 1302,\n",
       "         'thats': 1296,\n",
       "         'day': 1296,\n",
       "         'ihnen': 1294,\n",
       "         'geld': 1290,\n",
       "         'problem': 1284,\n",
       "         'anything': 1282,\n",
       "         'kannst': 1278,\n",
       "         'oder': 1264,\n",
       "         'dieser': 1259,\n",
       "         'viele': 1252,\n",
       "         'lot': 1249,\n",
       "         'right': 1242,\n",
       "         'jetzt': 1230,\n",
       "         'gerne': 1230,\n",
       "         'book': 1230,\n",
       "         'people': 1228,\n",
       "         'morgen': 1223,\n",
       "         'kein': 1222,\n",
       "         'gerade': 1213,\n",
       "         'einer': 1212,\n",
       "         'say': 1207,\n",
       "         'last': 1205,\n",
       "         'new': 1201,\n",
       "         'many': 1188,\n",
       "         'told': 1182,\n",
       "         'des': 1159,\n",
       "         'long': 1156,\n",
       "         'buch': 1137,\n",
       "         'hes': 1136,\n",
       "         'room': 1136,\n",
       "         'sagte': 1131,\n",
       "         'three': 1129,\n",
       "         'gibt': 1121,\n",
       "         'gesagt': 1120,\n",
       "         'house': 1117,\n",
       "         'franzosisch': 1112,\n",
       "         'does': 1106,\n",
       "         'drei': 1104,\n",
       "         'our': 1085,\n",
       "         'still': 1083,\n",
       "         'went': 1079,\n",
       "         'only': 1077,\n",
       "         'isnt': 1072,\n",
       "         'better': 1071,\n",
       "         'id': 1065,\n",
       "         'hause': 1055,\n",
       "         'always': 1053,\n",
       "         'wollte': 1053,\n",
       "         'made': 1049,\n",
       "         'give': 1048,\n",
       "         'arbeit': 1045,\n",
       "         'way': 1044,\n",
       "         'sehen': 1043,\n",
       "         'school': 1042,\n",
       "         'einmal': 1040,\n",
       "         'today': 1039,\n",
       "         'wer': 1030,\n",
       "         'speak': 1022,\n",
       "         'helfen': 1020,\n",
       "         'must': 1016,\n",
       "         'let': 1013,\n",
       "         'mussen': 1012,\n",
       "         'wirklich': 1004,\n",
       "         'put': 999,\n",
       "         'leben': 998,\n",
       "         'into': 998,\n",
       "         'off': 996,\n",
       "         'seinen': 985,\n",
       "         'dein': 982,\n",
       "         'before': 982,\n",
       "         'look': 971,\n",
       "         'thought': 967,\n",
       "         'sprechen': 965,\n",
       "         'haus': 963,\n",
       "         'old': 962,\n",
       "         'kommen': 960,\n",
       "         'zur': 955,\n",
       "         'ever': 953,\n",
       "         'again': 950,\n",
       "         'waren': 948,\n",
       "         'over': 946,\n",
       "         'talk': 945,\n",
       "         'asked': 941,\n",
       "         'wann': 940,\n",
       "         'ware': 935,\n",
       "         'two': 934,\n",
       "         'tag': 933,\n",
       "         'essen': 931,\n",
       "         'gesehen': 925,\n",
       "         'whats': 925,\n",
       "         'wissen': 917,\n",
       "         'glaube': 913,\n",
       "         'geht': 912,\n",
       "         'wieder': 908,\n",
       "         'love': 902,\n",
       "         'down': 901,\n",
       "         'eat': 900,\n",
       "         'habt': 899,\n",
       "         'leave': 887,\n",
       "         'lange': 886,\n",
       "         'lassen': 883,\n",
       "         'years': 882,\n",
       "         'vater': 882,\n",
       "         'mag': 881,\n",
       "         'besser': 881,\n",
       "         'other': 880,\n",
       "         'every': 873,\n",
       "         'gestern': 872,\n",
       "         'tomorrow': 870,\n",
       "         'meinen': 869,\n",
       "         'macht': 868,\n",
       "         'einfach': 863,\n",
       "         'auto': 856,\n",
       "         'or': 847,\n",
       "         'everything': 846,\n",
       "         'bis': 840,\n",
       "         'left': 839,\n",
       "         'sure': 839,\n",
       "         'father': 837,\n",
       "         'job': 834,\n",
       "         'kind': 824,\n",
       "         'after': 824,\n",
       "         'gemacht': 820,\n",
       "         'little': 817,\n",
       "         'find': 815,\n",
       "         'kommt': 814,\n",
       "         'willst': 808,\n",
       "         'lets': 806,\n",
       "         'schule': 806,\n",
       "         'mann': 801,\n",
       "         'wont': 801,\n",
       "         'auch': 800,\n",
       "         'ab': 797,\n",
       "         'wanted': 791,\n",
       "         'may': 790,\n",
       "         'keinen': 787,\n",
       "         'ins': 782,\n",
       "         'ob': 780,\n",
       "         'doch': 777,\n",
       "         'nothing': 772,\n",
       "         'next': 768,\n",
       "         'fast': 764,\n",
       "         'read': 763,\n",
       "         'zwei': 761,\n",
       "         'ging': 759,\n",
       "         'zimmer': 759,\n",
       "         'ask': 755,\n",
       "         'dog': 755,\n",
       "         'believe': 754,\n",
       "         'took': 751,\n",
       "         'name': 749,\n",
       "         'yesterday': 747,\n",
       "         'kam': 746,\n",
       "         'musst': 745,\n",
       "         'night': 736,\n",
       "         'ohne': 735,\n",
       "         'kinder': 735,\n",
       "         'bus': 734,\n",
       "         'uhr': 733,\n",
       "         'diesem': 728,\n",
       "         'stay': 724,\n",
       "         'doing': 724,\n",
       "         'mutter': 721,\n",
       "         'without': 720,\n",
       "         'getan': 718,\n",
       "         'hund': 715,\n",
       "         'sicher': 712,\n",
       "         'english': 709,\n",
       "         'children': 706,\n",
       "         'these': 704,\n",
       "         'frage': 704,\n",
       "         'marys': 704,\n",
       "         'wrong': 702,\n",
       "         'away': 695,\n",
       "         'couldnt': 693,\n",
       "         'buy': 689,\n",
       "         'first': 688,\n",
       "         'sollte': 688,\n",
       "         'saw': 687,\n",
       "         'their': 686,\n",
       "         'door': 681,\n",
       "         'done': 679,\n",
       "         'came': 677,\n",
       "         'keep': 676,\n",
       "         'seinem': 675,\n",
       "         'weit': 672,\n",
       "         'live': 670,\n",
       "         'theres': 668,\n",
       "         'diesen': 668,\n",
       "         'mother': 666,\n",
       "         'stop': 665,\n",
       "         'japan': 665,\n",
       "         'friends': 662,\n",
       "         'sei': 660,\n",
       "         'sollen': 659,\n",
       "         'seiner': 658,\n",
       "         'play': 657,\n",
       "         'them': 653,\n",
       "         'gehort': 651,\n",
       "         'even': 650,\n",
       "         'niemand': 650,\n",
       "         'happy': 648,\n",
       "         'plan': 647,\n",
       "         'life': 639,\n",
       "         'used': 638,\n",
       "         'call': 637,\n",
       "         'zuruck': 637,\n",
       "         'happened': 636,\n",
       "         'alone': 633,\n",
       "         'weg': 629,\n",
       "         'late': 628,\n",
       "         'dort': 625,\n",
       "         'sagt': 624,\n",
       "         'lost': 618,\n",
       "         'lass': 616,\n",
       "         'heard': 616,\n",
       "         'paar': 616,\n",
       "         'hilfe': 615,\n",
       "         'soon': 615,\n",
       "         'enough': 613,\n",
       "         'allein': 611,\n",
       "         'wusste': 608,\n",
       "         'lernen': 608,\n",
       "         'meinem': 607,\n",
       "         'jeden': 604,\n",
       "         'understand': 604,\n",
       "         'ganz': 600,\n",
       "         'wollen': 600,\n",
       "         'hand': 600,\n",
       "         'gern': 598,\n",
       "         'meiner': 598,\n",
       "         'ihren': 597,\n",
       "         'looking': 596,\n",
       "         'hard': 595,\n",
       "         'sah': 594,\n",
       "         'party': 594,\n",
       "         'found': 590,\n",
       "         'geben': 590,\n",
       "         'oft': 590,\n",
       "         'things': 590,\n",
       "         'solltest': 589,\n",
       "         'already': 589,\n",
       "         'feel': 587,\n",
       "         'soll': 587,\n",
       "         'selbst': 586,\n",
       "         'dachte': 586,\n",
       "         'wants': 581,\n",
       "         'seit': 579,\n",
       "         'denke': 577,\n",
       "         'often': 568,\n",
       "         'jemand': 567,\n",
       "         'try': 566,\n",
       "         'wasnt': 564,\n",
       "         'fragen': 563,\n",
       "         'bad': 562,\n",
       "         'gab': 562,\n",
       "         'each': 562,\n",
       "         'knows': 559,\n",
       "         'gave': 558,\n",
       "         'yet': 557,\n",
       "         'frau': 557,\n",
       "         'remember': 556,\n",
       "         'hatten': 556,\n",
       "         'damit': 554,\n",
       "         'sollten': 554,\n",
       "         'vielleicht': 553,\n",
       "         'wirst': 551,\n",
       "         'thing': 543,\n",
       "         'hope': 541,\n",
       "         'busy': 540,\n",
       "         'tur': 540,\n",
       "         'bought': 538,\n",
       "         'seid': 537,\n",
       "         'recht': 537,\n",
       "         'freund': 537,\n",
       "         'tut': 535,\n",
       "         'madchen': 533,\n",
       "         'morning': 531,\n",
       "         'seen': 531,\n",
       "         'englisch': 530,\n",
       "         'davon': 528,\n",
       "         'passiert': 528,\n",
       "         'week': 528,\n",
       "         'hear': 523,\n",
       "         'question': 523,\n",
       "         'bleiben': 520,\n",
       "         'deinen': 520,\n",
       "         'married': 519,\n",
       "         'true': 517,\n",
       "         'train': 517,\n",
       "         'answer': 516,\n",
       "         'friend': 516,\n",
       "         'idea': 516,\n",
       "         'youve': 514,\n",
       "         'spielen': 514,\n",
       "         'reden': 513,\n",
       "         'water': 513,\n",
       "         'leute': 513,\n",
       "         'around': 510,\n",
       "         'year': 510,\n",
       "         'wurden': 510,\n",
       "         'glauben': 509,\n",
       "         'wait': 508,\n",
       "         'best': 508,\n",
       "         'himself': 504,\n",
       "         'woche': 504,\n",
       "         'wish': 502,\n",
       "         'schnell': 501,\n",
       "         'because': 500,\n",
       "         'scheint': 498,\n",
       "         'fahren': 497,\n",
       "         'gute': 497,\n",
       "         'mind': 496,\n",
       "         'teacher': 496,\n",
       "         'few': 494,\n",
       "         'daruber': 493,\n",
       "         'same': 493,\n",
       "         'knew': 491,\n",
       "         'nacht': 491,\n",
       "         'jahre': 489,\n",
       "         'while': 489,\n",
       "         'cold': 487,\n",
       "         'else': 487,\n",
       "         'fehler': 486,\n",
       "         'warten': 484,\n",
       "         'bruder': 483,\n",
       "         'lie': 480,\n",
       "         'use': 479,\n",
       "         'able': 478,\n",
       "         'lesen': 474,\n",
       "         'menschen': 473,\n",
       "         'such': 472,\n",
       "         'open': 471,\n",
       "         'jahr': 471,\n",
       "         'anyone': 469,\n",
       "         'most': 469,\n",
       "         'stadt': 468,\n",
       "         'unter': 468,\n",
       "         'sieht': 468,\n",
       "         'likes': 466,\n",
       "         'lasst': 465,\n",
       "         'kaufen': 464,\n",
       "         'genug': 464,\n",
       "         'afraid': 463,\n",
       "         'zusammen': 463,\n",
       "         'coffee': 463,\n",
       "         'wouldnt': 463,\n",
       "         'anymore': 463,\n",
       "         'beim': 462,\n",
       "         'gegangen': 460,\n",
       "         'durch': 460,\n",
       "         'yourself': 460,\n",
       "         'spricht': 459,\n",
       "         'zug': 459,\n",
       "         'abend': 458,\n",
       "         'everyone': 458,\n",
       "         'horen': 457,\n",
       "         'konnten': 457,\n",
       "         'alt': 455,\n",
       "         'schwer': 455,\n",
       "         'someone': 455,\n",
       "         'watch': 452,\n",
       "         'drink': 452,\n",
       "         'being': 452,\n",
       "         'spat': 450,\n",
       "         'person': 450,\n",
       "         'big': 449,\n",
       "         'havent': 448,\n",
       "         'beautiful': 447,\n",
       "         'another': 446,\n",
       "         'letter': 446,\n",
       "         'jahren': 445,\n",
       "         'wasser': 444,\n",
       "         'days': 444,\n",
       "         'youll': 442,\n",
       "         'beschaftigt': 441,\n",
       "         'looked': 441,\n",
       "         'stand': 440,\n",
       "         'gekauft': 440,\n",
       "         'books': 440,\n",
       "         'wohl': 439,\n",
       "         'letzte': 439,\n",
       "         'almost': 437,\n",
       "         'shes': 436,\n",
       "         'freunde': 433,\n",
       "         'bed': 432,\n",
       "         'park': 432,\n",
       "         'seems': 429,\n",
       "         'looks': 429,\n",
       "         'gehe': 428,\n",
       "         'hattest': 428,\n",
       "         'write': 427,\n",
       "         'once': 427,\n",
       "         'meeting': 427,\n",
       "         'girl': 426,\n",
       "         'bald': 424,\n",
       "         'fertig': 424,\n",
       "         'learn': 424,\n",
       "         'brief': 423,\n",
       "         'tried': 422,\n",
       "         'gekommen': 421,\n",
       "         'both': 421,\n",
       "         'tennis': 421,\n",
       "         'chance': 421,\n",
       "         'wahr': 420,\n",
       "         'brother': 420,\n",
       "         'kaffee': 419,\n",
       "         'darf': 417,\n",
       "         'show': 415,\n",
       "         'finden': 414,\n",
       "         'glucklich': 413,\n",
       "         'bekommen': 411,\n",
       "         'getting': 411,\n",
       "         'boy': 408,\n",
       "         'parents': 408,\n",
       "         'food': 407,\n",
       "         'eltern': 406,\n",
       "         'hoffe': 405,\n",
       "         'says': 405,\n",
       "         'which': 405,\n",
       "         'might': 404,\n",
       "         'place': 404,\n",
       "         'hotel': 404,\n",
       "         'matter': 403,\n",
       "         'talking': 399,\n",
       "         'pay': 398,\n",
       "         'met': 398,\n",
       "         'tired': 398,\n",
       "         'treffen': 398,\n",
       "         'bucher': 397,\n",
       "         'unsere': 397,\n",
       "         'gefunden': 396,\n",
       "         'walk': 396,\n",
       "         'arent': 396,\n",
       "         'care': 395,\n",
       "         'turn': 395,\n",
       "         'world': 395,\n",
       "         'bett': 394,\n",
       "         'since': 393,\n",
       "         'daran': 392,\n",
       "         'happen': 392,\n",
       "         'lehrer': 391,\n",
       "         'nahm': 391,\n",
       "         'vergessen': 389,\n",
       "         'young': 386,\n",
       "         'sorry': 385,\n",
       "         'ihrer': 385,\n",
       "         'jeder': 385,\n",
       "         'ganze': 384,\n",
       "         'gefallt': 382,\n",
       "         'family': 381,\n",
       "         'anderen': 381,\n",
       "         'dazu': 380,\n",
       "         'weil': 380,\n",
       "         'junge': 379,\n",
       "         'genau': 379,\n",
       "         'doctor': 379,\n",
       "         'sleep': 378,\n",
       "         'waiting': 378,\n",
       "         'hours': 378,\n",
       "         'study': 377,\n",
       "         'dann': 376,\n",
       "         'denn': 376,\n",
       "         'accident': 376,\n",
       "         'augen': 375,\n",
       "         'vom': 374,\n",
       "         'liebe': 374,\n",
       "         'together': 373,\n",
       "         'baby': 373,\n",
       "         'ago': 373,\n",
       "         'brauchen': 372,\n",
       "         'andere': 371,\n",
       "         'schwester': 370,\n",
       "         'gewesen': 369,\n",
       "         'truth': 369,\n",
       "         'je': 369,\n",
       "         'angst': 367,\n",
       "         'great': 367,\n",
       "         'own': 367,\n",
       "         'ganzen': 366,\n",
       "         'station': 366,\n",
       "         'brauche': 365,\n",
       "         'gefallen': 364,\n",
       "         'einige': 364,\n",
       "         'mochtest': 364,\n",
       "         'until': 363,\n",
       "         'gar': 362,\n",
       "         'darauf': 362,\n",
       "         'verloren': 361,\n",
       "         'ten': 361,\n",
       "         'nehmen': 359,\n",
       "         'fragte': 358,\n",
       "         'meet': 355,\n",
       "         'schwimmen': 354,\n",
       "         'welt': 354,\n",
       "         'youd': 354,\n",
       "         'wunschte': 354,\n",
       "         'minutes': 354,\n",
       "         'died': 353,\n",
       "         'zehn': 353,\n",
       "         'early': 352,\n",
       "         'started': 352,\n",
       "         'alter': 352,\n",
       "         'uberhaupt': 352,\n",
       "         'quite': 351,\n",
       "         'phone': 351,\n",
       "         'geschichte': 351,\n",
       "         'sister': 350,\n",
       "         'arbeiten': 349,\n",
       "         'eines': 348,\n",
       "         'wahrheit': 347,\n",
       "         'easy': 346,\n",
       "         'schreiben': 345,\n",
       "         'wagen': 345,\n",
       "         'difficult': 345,\n",
       "         'bat': 344,\n",
       "         'mude': 343,\n",
       "         'favorite': 343,\n",
       "         'steht': 342,\n",
       "         'thank': 342,\n",
       "         'called': 341,\n",
       "         'groe': 341,\n",
       "         'computer': 341,\n",
       "         'coming': 340,\n",
       "         'moment': 340,\n",
       "         'lives': 340,\n",
       "         'bring': 339,\n",
       "         'fruh': 339,\n",
       "         'reading': 339,\n",
       "         'important': 339,\n",
       "         'tv': 338,\n",
       "         'worden': 338,\n",
       "         'fruher': 338,\n",
       "         'police': 337,\n",
       "         'marias': 337,\n",
       "         'kaum': 336,\n",
       "         'story': 336,\n",
       "         'trying': 335,\n",
       "         'ziemlich': 335,\n",
       "         'namen': 334,\n",
       "         'longer': 332,\n",
       "         'dies': 331,\n",
       "         'table': 331,\n",
       "         'those': 331,\n",
       "         'familie': 331,\n",
       "         'nobody': 330,\n",
       "         'musste': 330,\n",
       "         'strae': 330,\n",
       "         'usually': 329,\n",
       "         'nice': 327,\n",
       "         'neue': 327,\n",
       "         'working': 326,\n",
       "         'gro': 325,\n",
       "         'konntest': 325,\n",
       "         'gegessen': 324,\n",
       "         'deiner': 324,\n",
       "         'students': 323,\n",
       "         'wurdest': 323,\n",
       "         'beide': 322,\n",
       "         'stunden': 322,\n",
       "         'danke': 321,\n",
       "         'fun': 321,\n",
       "         'unser': 321,\n",
       "         'change': 320,\n",
       "         'office': 320,\n",
       "         'arzt': 319,\n",
       "         'meinung': 318,\n",
       "         'kenne': 317,\n",
       "         'lang': 317,\n",
       "         'times': 317,\n",
       "         'fenster': 317,\n",
       "         'letzten': 316,\n",
       "         'gleich': 315,\n",
       "         'leicht': 315,\n",
       "         'city': 315,\n",
       "         'rain': 314,\n",
       "         'japanese': 314,\n",
       "         'needs': 313,\n",
       "         'cat': 313,\n",
       "         'son': 313,\n",
       "         'cannot': 313,\n",
       "         'bank': 313,\n",
       "         'start': 312,\n",
       "         'eyes': 312,\n",
       "         'shouldnt': 312,\n",
       "         'playing': 312,\n",
       "         'dafur': 311,\n",
       "         'horte': 310,\n",
       "         'questions': 310,\n",
       "         'wife': 310,\n",
       "         'restaurant': 309,\n",
       "         'neuen': 308,\n",
       "         'sohn': 308,\n",
       "         'sorgen': 307,\n",
       "         'fell': 306,\n",
       "         'swim': 306,\n",
       "         'katze': 306,\n",
       "         'lieber': 306,\n",
       "         'bringen': 305,\n",
       "         'dollar': 304,\n",
       "         'warst': 304,\n",
       "         'eure': 304,\n",
       "         'gegen': 303,\n",
       "         'minuten': 303,\n",
       "         'forget': 302,\n",
       "         'myself': 301,\n",
       "         'braucht': 300,\n",
       "         'dinner': 300,\n",
       "         'land': 300,\n",
       "         'far': 299,\n",
       "         'drive': 299,\n",
       "         'hurt': 299,\n",
       "         'tisch': 299,\n",
       "         'hair': 299,\n",
       "         'picture': 299,\n",
       "         'halten': 298,\n",
       "         'leid': 297,\n",
       "         'tokyo': 297,\n",
       "         'word': 297,\n",
       "         'ran': 296,\n",
       "         'ihrem': 296,\n",
       "         'five': 296,\n",
       "         'john': 296,\n",
       "         'film': 294,\n",
       "         'mistake': 294,\n",
       "         'welche': 292,\n",
       "         'verstehen': 292,\n",
       "         'getroffen': 291,\n",
       "         'machte': 291,\n",
       "         'liegt': 290,\n",
       "         'bisschen': 290,\n",
       "         'funf': 289,\n",
       "         'number': 289,\n",
       "         'pretty': 289,\n",
       "         'country': 289,\n",
       "         'student': 288,\n",
       "         'nachsten': 288,\n",
       "         'weve': 287,\n",
       "         'wahrend': 287,\n",
       "         'mean': 286,\n",
       "         'finished': 286,\n",
       "         'under': 286,\n",
       "         'london': 285,\n",
       "         'through': 285,\n",
       "         'bevor': 285,\n",
       "         'mine': 284,\n",
       "         'gib': 284,\n",
       "         'window': 284,\n",
       "         'erst': 284,\n",
       "         'geschrieben': 283,\n",
       "         'angry': 282,\n",
       "         'wegen': 282,\n",
       "         'sick': 281,\n",
       "         'polizei': 281,\n",
       "         'fire': 280,\n",
       "         'hin': 280,\n",
       "         'krank': 280,\n",
       "         'guter': 280,\n",
       "         'child': 280,\n",
       "         'turned': 280,\n",
       "         'tage': 280,\n",
       "         'mach': 279,\n",
       "         'fish': 279,\n",
       "         'theyre': 279,\n",
       "         'living': 278,\n",
       "         'news': 277,\n",
       "         'wearing': 277,\n",
       "         'bahnhof': 277,\n",
       "         'pass': 276,\n",
       "         'half': 276,\n",
       "         'wort': 276,\n",
       "         'shouldve': 276,\n",
       "         'los': 274,\n",
       "         'gone': 274,\n",
       "         'decided': 274,\n",
       "         'trinken': 273,\n",
       "         'small': 273,\n",
       "         'trouble': 272,\n",
       "         'movie': 271,\n",
       "         'music': 270,\n",
       "         'spa': 269,\n",
       "         'versuchte': 269,\n",
       "         'brauchst': 269,\n",
       "         'seem': 268,\n",
       "         'run': 267,\n",
       "         'ende': 267,\n",
       "         'breakfast': 267,\n",
       "         'isst': 266,\n",
       "         'advice': 266,\n",
       "         'situation': 266,\n",
       "         'neues': 265,\n",
       "         'makes': 265,\n",
       "         'arm': 264,\n",
       "         'ja': 264,\n",
       "         'fahrt': 264,\n",
       "         'rauchen': 264,\n",
       "         'everybody': 264,\n",
       "         'taxi': 264,\n",
       "         'sag': 263,\n",
       "         'konnt': 263,\n",
       "         'lunch': 263,\n",
       "         'arrived': 263,\n",
       "         'alte': 263,\n",
       "         'walked': 261,\n",
       "         'month': 261,\n",
       "         'groer': 259,\n",
       "         'having': 259,\n",
       "         'versuchen': 258,\n",
       "         'fall': 258,\n",
       "         'town': 258,\n",
       "         'sofort': 258,\n",
       "         'komme': 256,\n",
       "         'killed': 256,\n",
       "         'caught': 256,\n",
       "         'wenig': 255,\n",
       "         'menge': 255,\n",
       "         'lied': 254,\n",
       "         'weiter': 254,\n",
       "         'broke': 253,\n",
       "         'sonst': 253,\n",
       "         'wohnt': 253,\n",
       "         'gewohnlich': 253,\n",
       "         'ready': 252,\n",
       "         'worry': 252,\n",
       "         'winter': 252,\n",
       "         'groen': 252,\n",
       "         'schlafen': 251,\n",
       "         'fahrrad': 251,\n",
       "         'mochten': 251,\n",
       "         'hot': 249,\n",
       "         'hungry': 249,\n",
       "         'street': 249,\n",
       "         'idee': 248,\n",
       "         'interested': 248,\n",
       "         'antwort': 248,\n",
       "         'miss': 247,\n",
       "         'goes': 247,\n",
       "         'nahe': 246,\n",
       "         'shoes': 246,\n",
       "         'versucht': 245,\n",
       "         'eating': 245,\n",
       "         'tat': 245,\n",
       "         'verlassen': 244,\n",
       "         'stupid': 244,\n",
       "         'gelesen': 244,\n",
       "         'spielt': 244,\n",
       "         'interesting': 244,\n",
       "         'glaubst': 243,\n",
       "         'woman': 243,\n",
       "         'river': 243,\n",
       "         'drauen': 242,\n",
       "         'high': 242,\n",
       "         'unfall': 242,\n",
       "         'stelle': 241,\n",
       "         'light': 241,\n",
       "         'sat': 240,\n",
       "         'grund': 240,\n",
       "         'older': 240,\n",
       "         'geworden': 240,\n",
       "         'komm': 239,\n",
       "         'arbeitet': 239,\n",
       "         'glad': 238,\n",
       "         'hands': 237,\n",
       "         'later': 237,\n",
       "         'einzige': 237,\n",
       "         'spater': 236,\n",
       "         'death': 236,\n",
       "         'exactly': 236,\n",
       "         'dollars': 236,\n",
       "         'hielt': 234,\n",
       "         'large': 234,\n",
       "         'besuchen': 234,\n",
       "         'tochter': 234,\n",
       "         'then': 233,\n",
       "         'stimmt': 233,\n",
       "         'jungen': 233,\n",
       "         'glaubt': 233,\n",
       "         'behind': 233,\n",
       "         'surprised': 233,\n",
       "         'become': 233,\n",
       "         'gegeben': 232,\n",
       "         'smoking': 232,\n",
       "         'known': 232,\n",
       "         'clothes': 232,\n",
       "         ...})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:37:02.313784Z",
     "start_time": "2024-11-29T19:37:02.283128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_word_freq = 3\n",
    "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "word_map = {k: v+1 for v, k in enumerate(words)}\n",
    "\n",
    "#adding special tokens\n",
    "word_map['<unk>'] = len(word_map) + 1\n",
    "word_map['<start>'] = len(word_map) + 1\n",
    "word_map['<end>'] = len(word_map) + 1\n",
    "word_map['<pad>'] = 0"
   ],
   "id": "36ae293b8bea4f38",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:37:02.329412Z",
     "start_time": "2024-11-29T19:37:02.313784Z"
    }
   },
   "cell_type": "code",
   "source": "len(word_map)",
   "id": "5bfba3be7a1a284c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16617"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:37:02.345498Z",
     "start_time": "2024-11-29T19:37:02.329412Z"
    }
   },
   "cell_type": "code",
   "source": "word_map['<unk>'], word_map['<start>'], word_map['<end>'], word_map['<pad>']",
   "id": "e0be444e14b063cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16614, 16615, 16616, 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:37:03.800451Z",
     "start_time": "2024-11-29T19:37:03.629106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('word_map_corpus.json', 'w') as j:\n",
    "\tjson.dump(word_map, j)"
   ],
   "id": "e60c31970446127e",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:37:04.400057Z",
     "start_time": "2024-11-29T19:37:04.372891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_enc_inp(words, word_map):\n",
    "\t\"\"\"\n",
    "\tEncode a list of words into a tensor of word ids.\n",
    "\t\n",
    "\tparameter:\n",
    "\twords: list of words in the sentences\n",
    "\tword_map: dictionary mapping words to indices\n",
    "\t\n",
    "\tReturns:\n",
    "\tlist: Encoded german words as a sequence of indices\n",
    "\t\"\"\"\n",
    "\tenc_c = [word_map.get(word, word_map['<unk>']) for word in words]\n",
    "\tenc_c += [word_map['<pad>']] * (max_len - len(words)) \n",
    "\treturn enc_c\n",
    "\n",
    "def encode_dec_inp(words, word_map):\n",
    "\t\"\"\"\n",
    "\tEncode translate into a seq of indices using a word-to-index mapping.\n",
    "\t\n",
    "\tparameter:\n",
    "\twords: list of words in the translate sentences\n",
    "\tword_map: dictionary mapping words to indices\n",
    "\t\n",
    "\tReturns:\n",
    "\t\"\"\"\n",
    "\tenc_c = ([word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \n",
    "\t\t\t [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n",
    "\t\t\t ) \n",
    "\treturn enc_c"
   ],
   "id": "edee8655a9b26df2",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:36:35.364289Z",
     "start_time": "2024-11-28T23:36:27.097980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#initialize an empty list to store encoded translate pairs\n",
    "pairs_encoded = []\n",
    "for pair in clean_pairs:\n",
    "\tenglish = encode_enc_inp(pair[0], word_map)\n",
    "\tgerman = encode_dec_inp(pair[1], word_map)\n",
    "\tpairs_encoded.append([english, german])"
   ],
   "id": "5800ea035ceed2f5",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:36:35.406754Z",
     "start_time": "2024-11-28T23:36:35.365197Z"
    }
   },
   "cell_type": "code",
   "source": "clean_pairs[1]",
   "id": "4b99dcacbda11930",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hi', 'gru gott'], dtype='<U370')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:36:35.439632Z",
     "start_time": "2024-11-28T23:36:35.406754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print a sample of the encoded pairs to verify\n",
    "print(\"Sample Encoded Pair:\")\n",
    "print(f\"English: {pairs_encoded[0][0]}\")\n",
    "print(f\"German: {pairs_encoded[0][1]}\")\n"
   ],
   "id": "24778437f61917f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Encoded Pair:\n",
      "English: [16614, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "German: [16615, 16614, 207, 16614, 16614, 5266, 16616, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:06.095268Z",
     "start_time": "2024-11-28T23:36:35.442284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#saving number coded wordmap\n",
    "f_name = 'pairs_encoded.json'\n",
    "with open(f_name, 'w') as p:\n",
    "\tjson.dump(pairs_encoded, p)"
   ],
   "id": "2b49bab8682eae1c",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:06.111252Z",
     "start_time": "2024-11-28T23:37:06.095268Z"
    }
   },
   "cell_type": "code",
   "source": "max_len = 100  # This should be fixed and consistent across training",
   "id": "c77332b772ee3f86",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:09.544138Z",
     "start_time": "2024-11-28T23:37:06.114544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.pairs = json.load(open('pairs_encoded.json'))\n",
    "        self.dataset_size = len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        enc_inp = torch.LongTensor(self.pairs[index][0])\n",
    "        dec = torch.LongTensor(self.pairs[index][1])\n",
    "        \n",
    "        dec_inp = dec[:-1]\n",
    "        dec_out = dec[1:]\n",
    "        \n",
    "        # Ensure all sequences are padded or truncated to max_len\n",
    "        enc_inp = enc_inp[:max_len]\n",
    "        dec_inp = dec_inp[:max_len]\n",
    "        dec_out = dec_out[:max_len]\n",
    "\n",
    "        return enc_inp, dec_inp, dec_out\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "train_data = TranslationDataset()\n"
   ],
   "id": "9f8eaeb533f2bd15",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:09.606408Z",
     "start_time": "2024-11-28T23:37:09.544138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    enc_inp, dec_inp, dec_out = zip(*batch)\n",
    "\n",
    "    # Pad all sequences to the fixed max_len\n",
    "    enc_inp = pad_sequence([torch.tensor(seq).long() for seq in enc_inp], batch_first=True, padding_value=word_map['<pad>'])\n",
    "    dec_inp = pad_sequence([torch.tensor(seq).long() for seq in dec_inp], batch_first=True, padding_value=word_map['<pad>'])\n",
    "    dec_out = pad_sequence([torch.tensor(seq).long() for seq in dec_out], batch_first=True, padding_value=word_map['<pad>'])\n",
    "\n",
    "    # Ensure padding is applied correctly for max_len\n",
    "    enc_inp = torch.cat([enc_inp, torch.full((enc_inp.size(0), max_len - enc_inp.size(1)), word_map['<pad>'], dtype=torch.long)], dim=1)\n",
    "    dec_inp = torch.cat([dec_inp, torch.full((dec_inp.size(0), max_len - dec_inp.size(1)), word_map['<pad>'], dtype=torch.long)], dim=1)\n",
    "    dec_out = torch.cat([dec_out, torch.full((dec_out.size(0), max_len - dec_out.size(1)), word_map['<pad>'], dtype=torch.long)], dim=1)\n",
    "\n",
    "    return enc_inp, dec_inp, dec_out\n"
   ],
   "id": "e01a0d388a835133",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:12.893212Z",
     "start_time": "2024-11-28T23:37:09.608954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = TranslationDataset()\n",
    "batch_size = 16  # Set batch size (adjust based on memory availability)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ],
   "id": "83f6bba4d287a573",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:13.066794Z",
     "start_time": "2024-11-28T23:37:12.893212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q_r=train_data[10]\n",
    "q_r"
   ],
   "id": "bdb2d83cef480f6c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([16614, 16614, 16614, 16614,  5266,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " tensor([16615, 16614,   207, 16614, 16614,  5266, 16616,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor([16614,   207, 16614, 16614,  5266, 16616,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:13.098389Z",
     "start_time": "2024-11-28T23:37:13.067456Z"
    }
   },
   "cell_type": "code",
   "source": "rev_word_map = {v: k for k,v in word_map.items()}",
   "id": "67d95594cab8e029",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:13.130131Z",
     "start_time": "2024-11-28T23:37:13.098389Z"
    }
   },
   "cell_type": "code",
   "source": "rev_word_map[5267]",
   "id": "2984bdd4df332c1d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gentle'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:13.146230Z",
     "start_time": "2024-11-28T23:37:13.130131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tensor_to_sentence(t, clean=False):\n",
    "\tq = t.detach().numpy()\n",
    "\tq_words = \" \".join([rev_word_map[v] for v in q])\n",
    "\tif clean:\n",
    "\t\tq_words = q_words.replace(\"<pad>\", \"\")\n",
    "\treturn q_words"
   ],
   "id": "b2a6df76057effd3",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:13.209204Z",
     "start_time": "2024-11-28T23:37:13.146230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q_words = tensor_to_sentence(q_r[0])\n",
    "r_words = tensor_to_sentence(q_r[1])\n",
    "q_words, r_words"
   ],
   "id": "e7b3cbfbb1f6120c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk> <unk> <unk> <unk> o <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>',\n",
       " '<start> <unk> a <unk> <unk> o <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:13.241087Z",
     "start_time": "2024-11-28T23:37:13.209204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, pad_id):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_id)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize embedding weights uniformly within [-0.1, 0.1].\"\"\"\n",
    "        initrange = 0.1\n",
    "        self.token_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to generate token embeddings.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor containing token indices.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor containing token embeddings.\n",
    "        \"\"\"\n",
    "        x_embed = self.token_embedding(x)\n",
    "        return x_embed\n"
   ],
   "id": "9b53fef8120d8ab4",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:13.272241Z",
     "start_time": "2024-11-28T23:37:13.241087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = self.pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)  # Get current sequence length\n",
    "        return self.pe[:, :seq_len].to(x.device)"
   ],
   "id": "434cbe8157ad4000",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:13.291238Z",
     "start_time": "2024-11-28T23:37:13.275600Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9c66229b53992cc9",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:13.323039Z",
     "start_time": "2024-11-28T23:37:13.291238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, max_len):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_size, padding_idx=vocab['<pad>'])\n",
    "        self.pos_embedding = PositionalEncoding(embed_size, max_len + 2)  # Including <start> and <end>\n",
    "        self.embed_size = embed_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        token_embed = self.token_embedding(x) * math.sqrt(self.embed_size)  # Scale embeddings\n",
    "        pos_embed = self.pos_embedding(x)[:, :x.size(1), :]\n",
    "        return token_embed + pos_embed"
   ],
   "id": "54f889f5242de7fa",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:37:13.370044Z",
     "start_time": "2024-11-28T23:37:13.323039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, vocab, d_model=512, n_heads=8, num_encoder_layers=6, num_decoder_layers=6,\n",
    "\t\t\t\t\t\t dim_feedforward=2048, dropout=0.1, max_len=15):\n",
    "\t\tsuper(Transformer, self).__init__()\n",
    "\t\tself.input_embedding = Embeddings(vocab, d_model, max_len)\n",
    "\t\tself.transformer = nn.Transformer(d_model=d_model,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  nhead=n_heads,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  num_encoder_layers=num_encoder_layers,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  num_decoder_layers=num_decoder_layers,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  dim_feedforward=dim_feedforward,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  dropout=dropout,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  batch_first=True)\n",
    "\t\tself.project_vocab_layer = nn.Linear(in_features=d_model, out_features=len(vocab))\n",
    "\t\tself.init_weights()\n",
    "\t\t\t\n",
    "\tdef init_weights(self):\n",
    "\t\tinitrange = 0.1\n",
    "\t\tself.project_vocab_layer.bias.data.zero_()\n",
    "\t\tself.project_vocab_layer.weight.data.uniform_(-initrange, initrange)\n",
    "\t\n",
    "\tdef forward(self, enc_input, dec_input):\n",
    "\t\tx_enc_embed = self.input_embedding(enc_input.long())  # (batch_size, enc_seq_len, d_model)\n",
    "\t\tx_dec_embed = self.input_embedding(dec_input.long())  # (batch_size, dec_seq_len, d_model)\n",
    "\t\t\n",
    "\t\tprint(f\"encoder embed shape : {x_enc_embed.size()}\")\n",
    "\t\tprint(f\"decoder embed shape : {x_dec_embed.size()}\")\n",
    "\t\t\t\n",
    "\t\t\t\t# Masks\n",
    "\t\tsrc_key_padding_mask = (enc_input == self.vocab['<pad>']).to(device)  # Shape: (batch_size, enc_seq_len)\n",
    "\t\ttgt_key_padding_mask = (dec_input == self.vocab['<pad>']).to(device)  # Shape: (batch_size, dec_seq_len)\n",
    "\t\t\t\n",
    "\t\t\t\t# Generate tgt_mask based on decoder input length\n",
    "\t\ttgt_mask = self.transformer.generate_square_subsequent_mask(dec_input.size(1)).to(enc_input.device)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Forward pass through transformer\n",
    "\t\tfeature = self.transformer(src=x_enc_embed,\n",
    "\t\t\t\t\t\t\t\t   tgt=x_dec_embed,\n",
    "\t\t\t\t\t\t\t\t   src_key_padding_mask=src_key_padding_mask,\n",
    "\t\t\t\t\t\t\t\t   tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "\t\t\t\t\t\t\t\t   memory_key_padding_mask=src_key_padding_mask,\n",
    "\t\t\t\t\t\t\t\t   tgt_mask=tgt_mask)\n",
    "\t\tlogits = self.project_vocab_layer(feature)\n",
    "\t\treturn logits"
   ],
   "id": "b4cf2034ebaf50b6",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T23:41:28.445326Z",
     "start_time": "2024-11-28T23:41:26.698175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Transformer(word_map, max_len=98).to(device)\n",
    "\n",
    "for i, (enc_inp, dec_inp, dec_out) in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(f\"Encoder input shape: {enc_inp.shape}\")\n",
    "    print(f\"Decoder input shape: {dec_inp.shape}\")\n",
    "    print(f\"Decoder output shape: {dec_out.shape}\")\n",
    "    \n",
    "    enc_inp, dec_inp, dec_out = enc_inp.to(device), dec_inp.to(device), dec_out.to(device)\n",
    "    \n",
    "    out = model(enc_inp, dec_inp)\n",
    "    print(f\"Model output shape: {out.shape}\")\n",
    "    \n",
    "    # Ensure shapes are consistent\n",
    "    assert enc_inp.shape[1] == dec_inp.shape[1], \"Mismatch in encoder and decoder sequence lengths!\"\n"
   ],
   "id": "60882cef8f2f17f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Encoder input shape: torch.Size([16, 100])\n",
      "Decoder input shape: torch.Size([16, 100])\n",
      "Decoder output shape: torch.Size([16, 100])\n",
      "encoder embed shape : torch.Size([16, 100, 512])\n",
      "decoder embed shape : torch.Size([16, 100, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smari\\AppData\\Local\\Temp\\ipykernel_13484\\481387600.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  enc_inp = pad_sequence([torch.tensor(seq).long() for seq in enc_inp], batch_first=True, padding_value=word_map['<pad>'])\n",
      "C:\\Users\\smari\\AppData\\Local\\Temp\\ipykernel_13484\\481387600.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dec_inp = pad_sequence([torch.tensor(seq).long() for seq in dec_inp], batch_first=True, padding_value=word_map['<pad>'])\n",
      "C:\\Users\\smari\\AppData\\Local\\Temp\\ipykernel_13484\\481387600.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dec_out = pad_sequence([torch.tensor(seq).long() for seq in dec_out], batch_first=True, padding_value=word_map['<pad>'])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Transformer' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 12\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDecoder output shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdec_out\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     10\u001B[0m enc_inp, dec_inp, dec_out \u001B[38;5;241m=\u001B[39m enc_inp\u001B[38;5;241m.\u001B[39mto(device), dec_inp\u001B[38;5;241m.\u001B[39mto(device), dec_out\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 12\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43menc_inp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdec_inp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel output shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mout\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Ensure shapes are consistent\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\transfer_learning\\anomalib_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\transfer_learning\\anomalib_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[32], line 30\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, enc_input, dec_input)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdecoder embed shape : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx_dec_embed\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     29\u001B[0m \t\t\u001B[38;5;66;03m# Masks\u001B[39;00m\n\u001B[1;32m---> 30\u001B[0m src_key_padding_mask \u001B[38;5;241m=\u001B[39m (enc_input \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<pad>\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mto(device)  \u001B[38;5;66;03m# Shape: (batch_size, enc_seq_len)\u001B[39;00m\n\u001B[0;32m     31\u001B[0m tgt_key_padding_mask \u001B[38;5;241m=\u001B[39m (dec_input \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<pad>\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mto(device)  \u001B[38;5;66;03m# Shape: (batch_size, dec_seq_len)\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \t\t\u001B[38;5;66;03m# Generate tgt_mask based on decoder input length\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\transfer_learning\\anomalib_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1709\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1707\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[0;32m   1708\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[1;32m-> 1709\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Transformer' object has no attribute 'vocab'"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5123683f98460aee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b56660c5a3562f55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b0454de74ac212a6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
